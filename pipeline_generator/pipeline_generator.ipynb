{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:41.114809100Z",
     "start_time": "2023-07-16T07:52:40.973811700Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Any, List, Dict\n",
    "\n",
    "from pyshacl import validate\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline generation algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ontology = get_ontology_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.671809Z",
     "start_time": "2023-07-16T07:52:41.108811500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Obtain Intent Information functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:50.458486Z",
     "start_time": "2023-07-15T13:00:50.270487Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_intent_iri(intent_graph):\n",
    "    intent_iri_query = f\"\"\"\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "SELECT ?iri\n",
    "WHERE {{\n",
    "    ?iri a dtbox:Intent .\n",
    "}}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(intent_iri_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['iri']\n",
    "\n",
    "\n",
    "def get_intent_dataset_problem(intent_graph, intent_iri):\n",
    "    dataset_problem_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?dataset ?problem\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:Intent .\n",
    "        {intent_iri.n3()} dtbox:overData ?dataset .\n",
    "        {intent_iri.n3()} dtbox:tackles ?problem .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(dataset_problem_query).bindings[0]\n",
    "    return result['dataset'], result['problem']\n",
    "\n",
    "\n",
    "def get_intent_params(intent_graph, intent_iri):\n",
    "    params_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?param ?value\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:UserIntent .\n",
    "        {intent_iri.n3()} dtbox:usingParameter ?param_value .\n",
    "        ?param_value dtbox:forParameter ?param .\n",
    "        ?param_value dtbox:has_value ?value .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(params_query).bindings\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_intent_info(intent_graph, intent_iri=None) -> Tuple[Any, Any, List[Any], Any]:\n",
    "    if not intent_iri:\n",
    "        intent_iri = get_intent_iri(intent_graph)\n",
    "\n",
    "    dataset, problem = get_intent_dataset_problem(intent_graph, intent_iri)\n",
    "    params = get_intent_params(intent_graph, intent_iri)\n",
    "\n",
    "    return dataset, problem, params, intent_iri"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.687335400Z",
     "start_time": "2023-07-16T07:52:42.676809Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Obtain Loader functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:55.379704800Z",
     "start_time": "2023-07-15T13:00:55.359705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nee3a405cc25d4048bd7801084c90ab48b101 http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://diviloper.dev/ontology#IOSpec\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#hasTag https://diviloper.dev/ontology/shapes#LabeledTabularDatasetShape\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#has_position 0\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.709334600Z",
     "start_time": "2023-07-16T07:52:42.687335400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Obtain Main component dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_implementation_input_specs(ontology, implementation):\n",
    "    input_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesInput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(input_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def get_implementation_output_specs(ontology, implementation):\n",
    "    output_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesOutput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(output_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def flatten_shape(graph, shape):\n",
    "    if (shape, SH['and'], None) in graph:\n",
    "        subshapes_query = f\"\"\"\n",
    "            PREFIX sh: <{SH}>\n",
    "            PREFIX rdf: <{RDF}>\n",
    "\n",
    "            SELECT ?subshape\n",
    "            WHERE {{\n",
    "                {shape.n3()} sh:and ?andNode .\n",
    "                ?andNode rdf:rest*/rdf:first ?subshape .\n",
    "            }}\n",
    "        \"\"\"\n",
    "        subshapes = graph.query(subshapes_query).bindings\n",
    "\n",
    "        return [x for subshape in subshapes for x in flatten_shape(graph, subshape['subshape'])]\n",
    "    else:\n",
    "        return [shape]\n",
    "\n",
    "\n",
    "def get_potential_implementations(ontology, problem_iri, intent_parameters=None) -> List[Tuple[Any, List[Any]]]:\n",
    "    if intent_parameters is None:\n",
    "        intent_parameters = []\n",
    "    intent_params_match = [f'dtbox:hasParameter {param.n3()} ;' for param in intent_parameters]\n",
    "    intent_params_separator = '            \\n'\n",
    "    main_implementation_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?implementation\n",
    "    WHERE {{\n",
    "        ?implementation a dtbox:Implementation ;\n",
    "            {intent_params_separator.join(intent_params_match)}\n",
    "            dtbox:implements ?algorithm .\n",
    "        ?algorithm a dtbox:Algorithm ;\n",
    "            dtbox:solves ?problem .\n",
    "        ?problem dtbox:subProblemOf* {problem_iri.n3()} .\n",
    "        FILTER NOT EXISTS{{\n",
    "            ?implementation a dtbox:ApplierImplementation.\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "    results = ontology.query(main_implementation_query).bindings\n",
    "    implementations = [result['implementation'] for result in results]\n",
    "\n",
    "    implementations_with_shapes = [\n",
    "        (implementation, get_implementation_input_specs(ontology, implementation))\n",
    "        for implementation in implementations]\n",
    "\n",
    "    return implementations_with_shapes\n",
    "\n",
    "\n",
    "def get_component_implementation(ontology, component):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:hasImplementation ?implementation .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['implementation']\n",
    "\n",
    "\n",
    "def get_implementation_components(ontology, implementation) -> List[Any]:\n",
    "    components_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?component\n",
    "        WHERE {{\n",
    "            ?component dtbox:hasImplementation {implementation.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(components_query).bindings\n",
    "    return [result['component'] for result in results]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.723335100Z",
     "start_time": "2023-07-16T07:52:42.713337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def find_components_to_satisfy_shape(ontology, shape, only_learners=True):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            ?implementation a dtbox:{'Learner' if only_learners else ''}Implementation ;\n",
    "                dtbox:specifiesOutput ?spec .\n",
    "            ?spec dtbox:hasTag {shape.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    implementations = [x['implementation'] for x in result]\n",
    "    components = [c\n",
    "                  for implementation in implementations\n",
    "                  for c in get_implementation_components(ontology, implementation)]\n",
    "    return components"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.758337Z",
     "start_time": "2023-07-16T07:52:42.717336400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def identify_data_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            if (io_shape, SH.targetClass, dmop.TabularDataset) in ontology:\n",
    "                return i if return_index else io_shapes\n",
    "\n",
    "\n",
    "def identify_model_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            query = f'''\n",
    "    PREFIX sh: <{SH}>\n",
    "    PREFIX rdfs: <{RDFS}>\n",
    "    PREFIX ddata: <{dd}>\n",
    "\n",
    "    ASK {{\n",
    "      {{\n",
    "        {io_shape.n3()} sh:targetClass ?targetClass .\n",
    "        ?targetClass rdfs:subClassOf* ddata:Model .\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        {io_shape.n3()} rdfs:subClassOf* ddata:Model .\n",
    "      }}\n",
    "    }}\n",
    "'''\n",
    "            if ontology.query(query).askAnswer:\n",
    "                return i if return_index else io_shapes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.759336Z",
     "start_time": "2023-07-16T07:52:42.738337700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: https://diviloper.dev/ontology/ABOX#component-decimal_scaling\n",
      "Implementation: https://diviloper.dev/ontology/ABOX#implementation-normalizer_(pmml)\n",
      "Specs: [[rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizedTabularDatasetShape')], [rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizerModel')]]\n",
      "Model spec: [rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizerModel')]\n"
     ]
    }
   ],
   "source": [
    "comp = dabox.term('component-decimal_scaling')\n",
    "print(f'Component: {comp}')\n",
    "impl = get_component_implementation(ontology, comp)\n",
    "print(f'Implementation: {impl}')\n",
    "specs = get_implementation_output_specs(ontology, impl)\n",
    "print(f'Specs: {specs}')\n",
    "model_spec = identify_model_io(ontology, specs)\n",
    "print(f'Model spec: {model_spec}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.939335900Z",
     "start_time": "2023-07-16T07:52:42.750336300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/ns/shacl#NodeShape\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://diviloper.dev/ontology#DataTag\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2002/07/owl#Thing\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/ns/shacl#targetClass https://diviloper.dev/ontology/Data#NormalizerModel\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/2002/07/owl#sameAs https://diviloper.dev/ontology/shapes#NormalizerModel\n"
     ]
    }
   ],
   "source": [
    "for s, p, o in ontology.triples((specs[1][0], None, None)):\n",
    "    print(f'{s} {p} {o}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.956335300Z",
     "start_time": "2023-07-16T07:52:42.941337900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def satisfies_shape(data_graph, shacl_graph, shape, focus):\n",
    "    conforms, g, report = validate(data_graph, shacl_graph=shacl_graph, validate_shapes=[shape], focus=focus)\n",
    "    return conforms\n",
    "\n",
    "\n",
    "def get_shape_target_class(ontology, shape):\n",
    "    return ontology.query(f\"\"\"\n",
    "        PREFIX sh: <{SH}>\n",
    "        SELECT ?targetClass\n",
    "        WHERE {{\n",
    "            <{shape}> sh:targetClass ?targetClass .\n",
    "        }}\n",
    "    \"\"\").bindings[0]['targetClass']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.999333100Z",
     "start_time": "2023-07-16T07:52:42.958335400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_implementation_parameters(ontology, implementation) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value ?order\n",
    "        WHERE {{\n",
    "            <{implementation}> dtbox:hasParameter ?parameter .\n",
    "            ?parameter dtbox:hasDefaultValue ?value ;\n",
    "                       dtbox:has_position ?order .\n",
    "        }}\n",
    "        ORDER BY ?order\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['order']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_overriden_parameters(ontology, component) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value ?position\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:overridesParameter ?parameterValue .\n",
    "            ?parameterValue dtbox:forParameter ?parameter ;\n",
    "                       dtbox:has_value ?value .\n",
    "            ?parameter dtbox:has_position ?position .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['position']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_parameters(ontology, component) -> Dict[URIRef, Tuple[Any, int]]:\n",
    "    implementation = get_component_implementation(ontology, component)\n",
    "    implementation_params = get_implementation_parameters(ontology, implementation)\n",
    "    component_params = get_component_overriden_parameters(ontology, component)\n",
    "    implementation_params.update(component_params)\n",
    "    return implementation_params\n",
    "\n",
    "def perform_param_substitution(graph, parameters, inputs):\n",
    "    for param in parameters.keys():\n",
    "        value, order = parameters[param]\n",
    "        if isinstance(value.value, str) and '$$LABEL$$' in value.value:\n",
    "            new_value = value.replace('$$LABEL$$', f'{get_inputs_label_name(graph, inputs)}')\n",
    "            parameters[param] = (Literal(new_value), order)\n",
    "    return parameters\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:42.999333100Z",
     "start_time": "2023-07-16T07:52:42.976338100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def add_step(graph, pipeline, task_name, implementation, parameters, order, previous_task=None, inputs=None,\n",
    "             outputs=None):\n",
    "    if outputs is None:\n",
    "        outputs = []\n",
    "    if inputs is None:\n",
    "        inputs = []\n",
    "    step = dw.term(task_name)\n",
    "    graph.add((pipeline, dtbox.hasStep, step))\n",
    "    graph.add((step, RDF.type, dtbox.Step))\n",
    "    graph.add((step, dtbox.runs, implementation))\n",
    "    graph.add((step, dtbox.has_position, Literal(order)))\n",
    "    for i, input in enumerate(inputs):\n",
    "        in_node = BNode()\n",
    "        graph.add((in_node, RDF.type, dtbox.IO))\n",
    "        graph.add((in_node, dtbox.hasData, input))\n",
    "        graph.add((in_node, dtbox.has_position, Literal(i)))\n",
    "        graph.add((step, dtbox.hasInput, in_node))\n",
    "    for o, output in enumerate(outputs):\n",
    "        out_node = BNode()\n",
    "        graph.add((out_node, RDF.type, dtbox.IO))\n",
    "        graph.add((out_node, dtbox.hasData, output))\n",
    "        graph.add((out_node, dtbox.has_position, Literal(o)))\n",
    "        graph.add((step, dtbox.hasOutput, out_node))\n",
    "    for parameter, (value, _) in parameters.items():\n",
    "        param_value = BNode()\n",
    "        graph.add((step, dtbox.hasParameterValue, param_value))\n",
    "        graph.add((param_value, dtbox.forParameter, parameter))\n",
    "        graph.add((param_value, dtbox.has_value, value))\n",
    "    if previous_task:\n",
    "        if isinstance(previous_task, list):\n",
    "            for previous in previous_task:\n",
    "                graph.add((previous, dtbox.followedBy, step))\n",
    "        else:\n",
    "            graph.add((previous_task, dtbox.followedBy, step))\n",
    "    return step"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:43.030332600Z",
     "start_time": "2023-07-16T07:52:42.992337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_component_transformations(ontology, component) -> List:\n",
    "    transformation_query = f'''\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?transformation\n",
    "        WHERE {{\n",
    "            <{component}> dtbox:hasTransformation ?transformation_list .\n",
    "            ?transformation_list rdf:rest*/rdf:first ?transformation .\n",
    "        }}\n",
    "    '''\n",
    "    transformations = ontology.query(transformation_query).bindings\n",
    "    return [x['transformation'] for x in transformations]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:43.043335700Z",
     "start_time": "2023-07-16T07:52:43.004336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_inputs_label_name(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    label_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isLabel true ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "\n",
    "        }}\n",
    "    \"\"\"\n",
    "    return graph.query(label_query).bindings[0]['label'].value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:43.066336800Z",
     "start_time": "2023-07-16T07:52:43.030332600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def copy_subgraph(source_graph: Graph, source_node: URIRef, destination_graph: Graph, destination_node: URIRef,\n",
    "                  replace_nodes: bool = True):\n",
    "    visited_nodes = set()\n",
    "    nodes_to_visit = [source_node]\n",
    "    mappings = {source_node: destination_node}\n",
    "\n",
    "    while nodes_to_visit:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        visited_nodes.add(current_node)\n",
    "        for predicate, object in source_graph.predicate_objects(current_node):\n",
    "            if predicate == OWL.sameAs:\n",
    "                continue\n",
    "            if replace_nodes and isinstance(object, IdentifiedNode):\n",
    "                if predicate == RDF.type or object in dmop:\n",
    "                    mappings[object] = object\n",
    "                else:\n",
    "                    if object not in visited_nodes:\n",
    "                        nodes_to_visit.append(object)\n",
    "                    if object not in mappings:\n",
    "                        mappings[object] = BNode()\n",
    "                destination_graph.add((mappings[current_node], predicate, mappings[object]))\n",
    "            else:\n",
    "                destination_graph.add((mappings[current_node], predicate, object))\n",
    "\n",
    "\n",
    "def annotate_io_with_spec(ontology: Graph, workflow_graph: Graph, io: URIRef, io_spec: List[URIRef]):\n",
    "    for spec in io_spec:\n",
    "        io_spec_class = next(ontology.objects(spec, SH.targetClass, True), None)\n",
    "        if io_spec_class is None or (io, RDF.type, io_spec_class) in workflow_graph:\n",
    "            continue\n",
    "        workflow_graph.add((io, RDF.type, io_spec_class))\n",
    "\n",
    "\n",
    "def annotate_ios_with_specs(ontology: Graph, workflow_graph: Graph, io: List[URIRef], specs: List[List[URIRef]]):\n",
    "    assert len(io) == len(specs), 'Number of IOs and specs must be the same'\n",
    "    for io, spec in zip(io, specs):\n",
    "        annotate_io_with_spec(ontology, workflow_graph, io, spec)\n",
    "\n",
    "\n",
    "def run_copy_transformation(ontology: Graph, workflow_graph: Graph, transformation, inputs, outputs):\n",
    "    input_index = next(ontology.objects(transformation, dtbox.copy_input, True)).value\n",
    "    output_index = next(ontology.objects(transformation, dtbox.copy_output, True)).value\n",
    "    input = inputs[input_index - 1]\n",
    "    output = outputs[output_index - 1]\n",
    "\n",
    "    copy_subgraph(workflow_graph, input, workflow_graph, output)\n",
    "\n",
    "\n",
    "def run_component_transformation(ontology: Graph, workflow_graph: Graph, component, inputs, outputs,\n",
    "                                 parameters: dict):\n",
    "    transformations = get_component_transformations(ontology, component)\n",
    "    for transformation in transformations:\n",
    "        if (transformation, RDF.type, dtbox.CopyTransformation) in ontology:\n",
    "            run_copy_transformation(ontology, workflow_graph, transformation, inputs, outputs)\n",
    "        else:\n",
    "            prefixes = f'''\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "PREFIX da: <{da}>\n",
    "PREFIX rdf: <{RDF}>\n",
    "PREFIX rdfs: <{RDFS}>\n",
    "PREFIX owl: <{OWL}>\n",
    "PREFIX xsd: <{XSD}>\n",
    "PREFIX dmop: <{dmop}>\n",
    "'''\n",
    "            query = next(ontology.objects(transformation, dtbox.transformation_query, True)).value\n",
    "            query = prefixes + query\n",
    "            for i in range(len(inputs)):\n",
    "                query = query.replace(f'$input{i + 1}', f'{inputs[i].n3()}')\n",
    "            for i in range(len(outputs)):\n",
    "                query = query.replace(f'$output{i + 1}', f'{outputs[i].n3()}')\n",
    "            for param, (value, order) in parameters.items():\n",
    "                query = query.replace(f'$param{order + 1}', f'{value.n3()}')\n",
    "                query = query.replace(f'$parameter{order + 1}', f'{value.n3()}')\n",
    "            workflow_graph.update(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:43.066336800Z",
     "start_time": "2023-07-16T07:52:43.045336800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def step_name(workflow_name, task_order, implementation):\n",
    "    return f'{workflow_name}-step_{task_order}_{implementation.fragment.replace(\"-\", \"_\")}'\n",
    "\n",
    "\n",
    "def build_workflow_train_test(workflow_name, ontology, dataset, main_component, split_component, transformations):\n",
    "    workflow_graph = get_graph()\n",
    "    workflow = dw.term(workflow_name)\n",
    "    workflow_graph.add((workflow, RDF.type, dtbox.Workflow))\n",
    "    task_order = 0\n",
    "\n",
    "    dataset_node = dw.term(f'{workflow_name}-original_dataset')\n",
    "\n",
    "    copy_subgraph(ontology, dataset, workflow_graph, dataset_node)\n",
    "\n",
    "    split_step_name = step_name(workflow_name, task_order, split_component)\n",
    "    split_outputs = [dw[f'{split_step_name}-output_train'], dw[f'{split_step_name}-output_test']]\n",
    "    split_parameters = get_component_parameters(ontology, split_component)\n",
    "    split_step = add_step(workflow_graph, workflow,\n",
    "                          split_step_name,\n",
    "                          split_component,\n",
    "                          split_parameters,\n",
    "                          task_order,\n",
    "                          None,\n",
    "                          [dataset_node],\n",
    "                          split_outputs)\n",
    "    run_component_transformation(ontology, workflow_graph, split_component,\n",
    "                                 [dataset_node], split_outputs,\n",
    "                                 split_parameters)\n",
    "\n",
    "    task_order += 1\n",
    "\n",
    "    train_dataset_node = split_outputs[0]\n",
    "    test_dataset_node = split_outputs[1]\n",
    "\n",
    "    previous_train_step = split_step\n",
    "    previous_test_step = split_step\n",
    "\n",
    "    for train_component in [*transformations, main_component]:\n",
    "        test_component = next(ontology.objects(train_component, dtbox.hasApplier, True), train_component)\n",
    "        same = train_component == test_component\n",
    "\n",
    "        train_step_name = step_name(workflow_name, task_order, train_component)\n",
    "        test_step_name = step_name(workflow_name, task_order + 1, test_component)\n",
    "\n",
    "        train_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                           get_component_implementation(ontology, train_component))\n",
    "        train_input_data_index = identify_data_io(ontology, train_input_specs, return_index=True)\n",
    "        train_transformation_inputs = [dw[f'{train_step_name}-input_{i}'] for i in range(len(train_input_specs))]\n",
    "        train_transformation_inputs[train_input_data_index] = train_dataset_node\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_inputs,\n",
    "                                train_input_specs)\n",
    "\n",
    "        train_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                             get_component_implementation(ontology, train_component))\n",
    "        train_output_model_index = identify_model_io(ontology, train_output_specs, return_index=True)\n",
    "        train_output_data_index = identify_data_io(ontology, train_output_specs, return_index=True)\n",
    "        train_transformation_outputs = [dw[f'{train_step_name}-output_{i}'] for i in range(len(train_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_outputs,\n",
    "                                train_output_specs)\n",
    "\n",
    "        train_parameters = get_component_parameters(ontology, train_component)\n",
    "        train_parameters = perform_param_substitution(workflow_graph, train_parameters, train_transformation_inputs)\n",
    "        train_step = add_step(workflow_graph, workflow,\n",
    "                              train_step_name,\n",
    "                              train_component, train_parameters, task_order, previous_train_step,\n",
    "                              train_transformation_inputs,\n",
    "                              train_transformation_outputs)\n",
    "\n",
    "        previous_train_step = train_step\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, train_component, train_transformation_inputs,\n",
    "                                     train_transformation_outputs, train_parameters)\n",
    "\n",
    "        if train_output_data_index is not None:\n",
    "            train_dataset_node = train_transformation_outputs[train_output_data_index]\n",
    "\n",
    "        task_order += 1\n",
    "\n",
    "        test_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                          get_component_implementation(ontology, test_component))\n",
    "        test_input_data_index = identify_data_io(ontology, test_input_specs, return_index=True)\n",
    "        test_input_model_index = identify_model_io(ontology, test_input_specs, return_index=True)\n",
    "        test_transformation_inputs = [dw[f'{test_step_name}-input_{i}'] for i in range(len(test_input_specs))]\n",
    "        test_transformation_inputs[test_input_data_index] = test_dataset_node\n",
    "        test_transformation_inputs[test_input_model_index] = train_transformation_outputs[train_output_model_index]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_inputs,\n",
    "                                test_input_specs)\n",
    "\n",
    "        test_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                            get_component_implementation(ontology, test_component))\n",
    "        test_output_data_index = identify_data_io(ontology, test_output_specs, return_index=True)\n",
    "        test_transformation_outputs = [dw[f'{test_step_name}-output_{i}'] for i in range(len(test_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_outputs,\n",
    "                                test_output_specs)\n",
    "\n",
    "        previous_test_steps = [previous_test_step, train_step] if not same else [previous_test_step]\n",
    "        test_parameters = get_component_parameters(ontology, test_component)\n",
    "        test_parameters = perform_param_substitution(workflow_graph, test_parameters, test_transformation_inputs)\n",
    "        test_step = add_step(workflow_graph, workflow,\n",
    "                             test_step_name,\n",
    "                             test_component, test_parameters, task_order, previous_test_steps,\n",
    "                             test_transformation_inputs,\n",
    "                             test_transformation_outputs)\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, test_component, test_transformation_inputs,\n",
    "                                     test_transformation_outputs, test_parameters)\n",
    "\n",
    "        test_dataset_node = test_transformation_outputs[test_output_data_index]\n",
    "        previous_test_step = test_step\n",
    "        task_order += 1\n",
    "\n",
    "    return workflow_graph, workflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:43.080337200Z",
     "start_time": "2023-07-16T07:52:43.064335500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=N4453aa96a5494619a8b57eb63a6e9528 (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_graph = get_graph()\n",
    "ins = Namespace('https://diviloper.dev/intent#')\n",
    "intent_graph.add((ins.DescriptionIntent, RDF.type, dtbox.Intent))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.overData, dd.term('penguins.csv')))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.tackles, dabox.Description))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:43.125332700Z",
     "start_time": "2023-07-16T07:52:43.082335400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "log = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:52:43.137335100Z",
     "start_time": "2023-07-16T07:52:43.097334900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: penguins.csv\n",
      "Problem: Description\n",
      "Intent params: []\n",
      "-------------------------------------------------\n",
      "Component: component-decision_tree_learner (implementation-decision_tree_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape']\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "-------------------------------------------------\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n",
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 0: workflow_0_DescriptionIntent_dca0b65b_eb5d_47b4_a49a_cc14072d9280\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 1: workflow_1_DescriptionIntent_09423af6_0368_46dd_90d7_c032af990811\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 2: workflow_2_DescriptionIntent_5c238fd5_9f2d_4f13_914d_4b8ffa656db7\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 3: workflow_3_DescriptionIntent_c3ac118a_b450_4831_9793_25a2b1eb6453\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 4: workflow_4_DescriptionIntent_918e5e82_add7_4581_bb8d_f363042f1020\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 5: workflow_5_DescriptionIntent_8d5e86ba_f92a_413b_92b1_93604e0109c3\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 6: workflow_6_DescriptionIntent_0ed6fc85_9b5a_497e_b7af_9d7926695287\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 7: workflow_7_DescriptionIntent_493df512_4120_44b5_b239_8e7a871fc2e5\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 8: workflow_8_DescriptionIntent_c782a391_89b9_4562_8bf1_db0b8fd9bf7b\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 9: workflow_9_DescriptionIntent_bf0dec65_920a_46fb_a7a8_fea412b68289\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 10: workflow_10_DescriptionIntent_a95ded66_0239_41ce_9543_f082ee9ef408\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 11: workflow_11_DescriptionIntent_1041532c_b6e6_4a2b_b80d_1fd642134f3e\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 12: workflow_12_DescriptionIntent_2f3647e9_e051_4929_a41f_71d1eb02a0c3\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 13: workflow_13_DescriptionIntent_3389edf8_507a_44a3_92c8_61933855fe8e\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 14: workflow_14_DescriptionIntent_db8635c5_5c70_4c3f_b825_ea2a4cbf2a0e\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 15: workflow_15_DescriptionIntent_f8168896_17e0_44f8_a678_2f9f5e240ff4\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 16: workflow_16_DescriptionIntent_bde1bc80_8ad9_4c8e_b94f_c7b619d02b0f\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 17: workflow_17_DescriptionIntent_fb6ea8a6_72ff_4f28_bfde_b0fc02108620\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 18: workflow_18_DescriptionIntent_ccaa0db9_f94e_426e_b112_fb7ea23a56b3\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 19: workflow_19_DescriptionIntent_c39e0eb9_deff_4de4_a422_bd7d2b14bca4\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 20: workflow_20_DescriptionIntent_93be2b07_04b6_4f1f_8237_794dbcb6d158\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 21: workflow_21_DescriptionIntent_03a7b784_f934_49f2_b4c4_13535709990c\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 22: workflow_22_DescriptionIntent_1d528e2a_1262_4864_9e97_34e97155a3b6\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 23: workflow_23_DescriptionIntent_47d18003_8956_4d37_9ccf_b02681a2e7fc\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n",
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 24: workflow_24_DescriptionIntent_15bfb9d9_c07f_4ea7_a1a3_bd9a1b6b3519\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 25: workflow_25_DescriptionIntent_6453503d_6c01_4fb1_9980_18dae0cdad7a\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 26: workflow_26_DescriptionIntent_fb37d9b1_2a5a_4f2a_a018_9de17b8a42e6\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 27: workflow_27_DescriptionIntent_d4524d98_375f_43d3_9ece_affc8cb1365e\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 28: workflow_28_DescriptionIntent_0786aa4f_4840_4384_ae46_82ff932867eb\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 29: workflow_29_DescriptionIntent_57fcb08c_5f87_44e0_a0df_8c148ae3bcba\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 30: workflow_30_DescriptionIntent_f2fcb144_7906_4d09_85e8_2ce2a5594de0\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 31: workflow_31_DescriptionIntent_b5d22ee0_db47_4bae_be38_3aba7bf4ee9a\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 32: workflow_32_DescriptionIntent_57e5c9f0_c584_4f77_9a03_897a759debe0\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 33: workflow_33_DescriptionIntent_1cdf8790_fc89_46d1_9853_102cd38f9549\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 34: workflow_34_DescriptionIntent_4894b7e5_8753_416a_a5fa_39dd2e9ed40f\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 35: workflow_35_DescriptionIntent_98ca82a2_b41f_47c1_9c91_a8a7d0a439b7\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 36: workflow_36_DescriptionIntent_74e3972b_c928_4cca_aa31_482c1dc711d2\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 37: workflow_37_DescriptionIntent_1d030e54_fcb9_4dec_b4a4_38b2a23ef349\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 38: workflow_38_DescriptionIntent_2624aa69_18c0_4292_9bdb_5e0ec81fff55\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 39: workflow_39_DescriptionIntent_af8c345a_214a_4058_b7b1_f440e0c58beb\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 40: workflow_40_DescriptionIntent_cd8b52a5_bbb2_4e2a_8392_32ceed08dd70\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 41: workflow_41_DescriptionIntent_8fadfc21_bd68_40e2_83fb_7240341712db\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 42: workflow_42_DescriptionIntent_ea56ac0b_53e0_4621_86c8_ffffaee72e5a\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 43: workflow_43_DescriptionIntent_e09cfb4d_0795_43ce_a876_ef99cf1a09f1\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 44: workflow_44_DescriptionIntent_d3b573fa_564a_4c6d_a96c_142d194d05d4\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 45: workflow_45_DescriptionIntent_5e96c368_c0df_4166_840b_374c56527ca6\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 46: workflow_46_DescriptionIntent_52ce09de_9136_4b9d_bfb3_6309de396541\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 47: workflow_47_DescriptionIntent_c22bc9f8_84e6_4b7c_b975_47d37d57dd64\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n",
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 48: workflow_48_DescriptionIntent_ff0fb3f8_feb8_45c9_ba7d_61c15ffea78e\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 49: workflow_49_DescriptionIntent_2c2e758f_6e0d_4031_b7b6_5cca4112d2e8\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 50: workflow_50_DescriptionIntent_f8d2bafd_f343_487d_ab01_1769c2f145ef\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 51: workflow_51_DescriptionIntent_cfcd1668_f7fb_46c8_80e7_e6c7bf01cb28\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 52: workflow_52_DescriptionIntent_8c6f313d_fae5_4a90_a9aa_a15703af0955\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 53: workflow_53_DescriptionIntent_2fd6ab41_85e4_4555_a6a5_d4631ca22705\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 54: workflow_54_DescriptionIntent_9767a8c8_aade_47a6_8702_841f30901e0e\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 55: workflow_55_DescriptionIntent_98ad508c_c712_4c8f_b6cd_cd324e0be0be\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 56: workflow_56_DescriptionIntent_f42b83eb_bce5_4374_a75a_d5a5857d4544\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 57: workflow_57_DescriptionIntent_e61caff3_6118_4fb1_96d0_4039027a4ea6\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 58: workflow_58_DescriptionIntent_4fa44c6e_05d1_498c_b3e4_d1de37078ec7\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 59: workflow_59_DescriptionIntent_af9e24ae_21e5_4566_896c_428726a73570\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 60: workflow_60_DescriptionIntent_bd9e131f_f5b9_4404_8477_68c3f1a93ab5\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 61: workflow_61_DescriptionIntent_df80b74c_9cd9_4278_9de7_6a691004439a\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 62: workflow_62_DescriptionIntent_95cc63f1_af61_4c65_9101_a4a6ab4c97ce\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 63: workflow_63_DescriptionIntent_843ef212_eb50_4a42_a2c8_62198d4fe9cd\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 64: workflow_64_DescriptionIntent_d7036a98_aaf9_4a40_ae95_c67a14de3ce3\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 65: workflow_65_DescriptionIntent_6f0f8079_4c48_4b78_ac13_8537be139d21\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 66: workflow_66_DescriptionIntent_cc98f845_28e0_4b6a_b395_818841187c4a\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 67: workflow_67_DescriptionIntent_a44f7dbe_5f5c_49d1_a001_e93aa447eb24\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 68: workflow_68_DescriptionIntent_088ecb67_7d91_4210_b679_3a58c171e4f6\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 69: workflow_69_DescriptionIntent_1d40259d_e5bc_4f08_8d11_dff1aa8460ed\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 70: workflow_70_DescriptionIntent_c7fd77c8_77d4_44ab_9cf6_967dc16a9960\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 71: workflow_71_DescriptionIntent_b94e65ca_d8a0_4e4a_adde_0ae4e7daf443\n"
     ]
    }
   ],
   "source": [
    "dataset, problem, intent_params, intent_iri = get_intent_info(intent_graph)\n",
    "folder = f'./workflows/{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "if log:\n",
    "    print(f'Dataset: {dataset.fragment}')\n",
    "    print(f'Problem: {problem.fragment}')\n",
    "    print(f'Intent params: {intent_params}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "comps = get_potential_implementations(ontology, problem, [x['param'] for x in intent_params])\n",
    "components = [\n",
    "    (c, impl, inputs)\n",
    "    for impl, inputs in comps\n",
    "    for c in get_implementation_components(ontology, impl)\n",
    "]\n",
    "if log:\n",
    "    for component, implementation, inputs in components:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "        for im_input in inputs:\n",
    "            print(f'\\tInput: {[x.fragment for x in im_input]}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "workflow_order = 0\n",
    "\n",
    "split_components = [\n",
    "    dabox.term('component-random_absolute_train_test_split'),\n",
    "    dabox.term('component-random_relative_train_test_split'),\n",
    "    dabox.term('component-top_k_absolute_train_test_split'),\n",
    "    dabox.term('component-top_k_relative_train_test_split'),\n",
    "]\n",
    "\n",
    "for component, implementation, inputs in components[1:]:\n",
    "    if log:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "    shapes_to_satisfy = identify_data_io(ontology, inputs)\n",
    "    assert shapes_to_satisfy is not None and len(shapes_to_satisfy) > 0\n",
    "    if log:\n",
    "        print(f'\\tData input: {[x.fragment for x in shapes_to_satisfy]}')\n",
    "\n",
    "    unsatisfied_shapes = [shape for shape in shapes_to_satisfy if\n",
    "                          not satisfies_shape(ontology, ontology, shape, dataset)]\n",
    "\n",
    "    available_transformations = {\n",
    "        shape: find_components_to_satisfy_shape(ontology, shape, only_learners=True)\n",
    "        for shape in unsatisfied_shapes\n",
    "    }\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tUnsatisfied shapes: ')\n",
    "        for shape, comps in available_transformations.items():\n",
    "            print(f'\\t\\t{shape.fragment}: {[x.fragment for x in comps]}')\n",
    "\n",
    "    transformation_combinations = list(itertools.product(split_components, *available_transformations.values()))\n",
    "    # TODO - check if the combination is valid and whether further transformations are needed\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tTotal combinations: {len(transformation_combinations)}')\n",
    "\n",
    "    for i, transformation_combination in enumerate(transformation_combinations):\n",
    "        if log:\n",
    "            print(\n",
    "                f'\\t\\tCombination {i + 1} / {len(transformation_combinations)}: {[x.fragment for x in transformation_combination]}')\n",
    "\n",
    "        workflow_name = f'workflow_{workflow_order}_{intent_iri.fragment}_{uuid.uuid4()}'.replace('-', '_')\n",
    "        wg, w = build_workflow_train_test(workflow_name, ontology, dataset, component, transformation_combination[0],\n",
    "                                          transformation_combination[1:])\n",
    "        if log:\n",
    "            print(f'\\t\\tWorkflow {workflow_order}: {w.fragment}')\n",
    "        wg.serialize(f'{folder}{workflow_name}.ttl', format='turtle')\n",
    "        workflow_order += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:53:19.207958700Z",
     "start_time": "2023-07-16T07:52:43.125332700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
