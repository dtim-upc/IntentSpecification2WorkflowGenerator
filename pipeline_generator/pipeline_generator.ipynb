{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:48.307894200Z",
     "start_time": "2023-07-15T18:19:48.205892500Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import uuid\n",
    "from typing import Tuple, Any, List, Dict\n",
    "\n",
    "from pyshacl import validate\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline generation algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ontology = get_ontology_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:49.843403800Z",
     "start_time": "2023-07-15T18:19:48.309892400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Obtain Intent Information functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:50.458486Z",
     "start_time": "2023-07-15T13:00:50.270487Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_intent_iri(intent_graph):\n",
    "    intent_iri_query = f\"\"\"\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "SELECT ?iri\n",
    "WHERE {{\n",
    "    ?iri a dtbox:Intent .\n",
    "}}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(intent_iri_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['iri']\n",
    "\n",
    "\n",
    "def get_intent_dataset_problem(intent_graph, intent_iri):\n",
    "    dataset_problem_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?dataset ?problem\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:Intent .\n",
    "        {intent_iri.n3()} dtbox:overData ?dataset .\n",
    "        {intent_iri.n3()} dtbox:tackles ?problem .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(dataset_problem_query).bindings[0]\n",
    "    return result['dataset'], result['problem']\n",
    "\n",
    "\n",
    "def get_intent_params(intent_graph, intent_iri):\n",
    "    params_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?param ?value\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:UserIntent .\n",
    "        {intent_iri.n3()} dtbox:usingParameter ?param_value .\n",
    "        ?param_value dtbox:forParameter ?param .\n",
    "        ?param_value dtbox:has_value ?value .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(params_query).bindings\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_intent_info(intent_graph, intent_iri=None) -> Tuple[Any, Any, List[Any], Any]:\n",
    "    if not intent_iri:\n",
    "        intent_iri = get_intent_iri(intent_graph)\n",
    "\n",
    "    dataset, problem = get_intent_dataset_problem(intent_graph, intent_iri)\n",
    "    params = get_intent_params(intent_graph, intent_iri)\n",
    "\n",
    "    return dataset, problem, params, intent_iri"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:49.857406800Z",
     "start_time": "2023-07-15T18:19:49.847404800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Obtain Loader functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:55.379704800Z",
     "start_time": "2023-07-15T13:00:55.359705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nee3a405cc25d4048bd7801084c90ab48b101 http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://diviloper.dev/ontology#IOSpec\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#hasTag https://diviloper.dev/ontology/shapes#LabeledTabularDatasetShape\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#has_position 0\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:49.885405600Z",
     "start_time": "2023-07-15T18:19:49.859403500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Obtain Main component dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_implementation_input_specs(ontology, implementation):\n",
    "    input_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesInput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(input_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def get_implementation_output_specs(ontology, implementation):\n",
    "    output_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesOutput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(output_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def flatten_shape(graph, shape):\n",
    "    if (shape, SH['and'], None) in graph:\n",
    "        subshapes_query = f\"\"\"\n",
    "            PREFIX sh: <{SH}>\n",
    "            PREFIX rdf: <{RDF}>\n",
    "\n",
    "            SELECT ?subshape\n",
    "            WHERE {{\n",
    "                {shape.n3()} sh:and ?andNode .\n",
    "                ?andNode rdf:rest*/rdf:first ?subshape .\n",
    "            }}\n",
    "        \"\"\"\n",
    "        subshapes = graph.query(subshapes_query).bindings\n",
    "\n",
    "        return [x for subshape in subshapes for x in flatten_shape(graph, subshape['subshape'])]\n",
    "    else:\n",
    "        return [shape]\n",
    "\n",
    "\n",
    "def get_potential_implementations(ontology, problem_iri, intent_parameters=None) -> List[Tuple[Any, List[Any]]]:\n",
    "    if intent_parameters is None:\n",
    "        intent_parameters = []\n",
    "    intent_params_match = [f'dtbox:hasParameter {param.n3()} ;' for param in intent_parameters]\n",
    "    intent_params_separator = '            \\n'\n",
    "    main_implementation_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?implementation\n",
    "    WHERE {{\n",
    "        ?implementation a dtbox:Implementation ;\n",
    "            {intent_params_separator.join(intent_params_match)}\n",
    "            dtbox:implements ?algorithm .\n",
    "        ?algorithm a dtbox:Algorithm ;\n",
    "            dtbox:solves ?problem .\n",
    "        ?problem dtbox:subProblemOf* {problem_iri.n3()} .\n",
    "        FILTER NOT EXISTS{{\n",
    "            ?implementation a dtbox:ApplierImplementation.\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "    results = ontology.query(main_implementation_query).bindings\n",
    "    implementations = [result['implementation'] for result in results]\n",
    "\n",
    "    implementations_with_shapes = [\n",
    "        (implementation, get_implementation_input_specs(ontology, implementation))\n",
    "        for implementation in implementations]\n",
    "\n",
    "    return implementations_with_shapes\n",
    "\n",
    "\n",
    "def get_component_implementation(ontology, component):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:hasImplementation ?implementation .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['implementation']\n",
    "\n",
    "\n",
    "def get_implementation_components(ontology, implementation) -> List[Any]:\n",
    "    components_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?component\n",
    "        WHERE {{\n",
    "            ?component dtbox:hasImplementation {implementation.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(components_query).bindings\n",
    "    return [result['component'] for result in results]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:49.888405Z",
     "start_time": "2023-07-15T18:19:49.883407700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def find_components_to_satisfy_shape(ontology, shape, only_learners=True):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            ?implementation a dtbox:{'Learner' if only_learners else ''}Implementation ;\n",
    "                dtbox:specifiesOutput ?spec .\n",
    "            ?spec dtbox:hasTag {shape.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    implementations = [x['implementation'] for x in result]\n",
    "    components = [c\n",
    "                  for implementation in implementations\n",
    "                  for c in get_implementation_components(ontology, implementation)]\n",
    "    return components"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:49.904404600Z",
     "start_time": "2023-07-15T18:19:49.891404900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def identify_data_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            if (io_shape, SH.targetClass, dmop.TabularDataset) in ontology:\n",
    "                return i if return_index else io_shapes\n",
    "\n",
    "\n",
    "def identify_model_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            query = f'''\n",
    "    PREFIX sh: <{SH}>\n",
    "    PREFIX rdfs: <{RDFS}>\n",
    "    PREFIX ddata: <{dd}>\n",
    "\n",
    "    ASK {{\n",
    "      {{\n",
    "        {io_shape.n3()} sh:targetClass ?targetClass .\n",
    "        ?targetClass rdfs:subClassOf* ddata:Model .\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        {io_shape.n3()} rdfs:subClassOf* ddata:Model .\n",
    "      }}\n",
    "    }}\n",
    "'''\n",
    "            if ontology.query(query).askAnswer:\n",
    "                return i if return_index else io_shapes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:49.938404200Z",
     "start_time": "2023-07-15T18:19:49.909406100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: https://diviloper.dev/ontology/ABOX#component-decimal_scaling\n",
      "Implementation: https://diviloper.dev/ontology/ABOX#implementation-normalizer_(pmml)\n",
      "Specs: [[rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizedTabularDatasetShape')], [rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizerModel')]]\n",
      "Model spec: [rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizerModel')]\n"
     ]
    }
   ],
   "source": [
    "comp = dabox.term('component-decimal_scaling')\n",
    "print(f'Component: {comp}')\n",
    "impl = get_component_implementation(ontology, comp)\n",
    "print(f'Implementation: {impl}')\n",
    "specs = get_implementation_output_specs(ontology, impl)\n",
    "print(f'Specs: {specs}')\n",
    "model_spec = identify_model_io(ontology, specs)\n",
    "print(f'Model spec: {model_spec}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.093404600Z",
     "start_time": "2023-07-15T18:19:49.937407200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/ns/shacl#NodeShape\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://diviloper.dev/ontology#DataTag\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2002/07/owl#Thing\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/ns/shacl#targetClass https://diviloper.dev/ontology/Data#NormalizerModel\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/2002/07/owl#sameAs https://diviloper.dev/ontology/shapes#NormalizerModel\n"
     ]
    }
   ],
   "source": [
    "for s, p, o in ontology.triples((specs[1][0], None, None)):\n",
    "    print(f'{s} {p} {o}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.109403800Z",
     "start_time": "2023-07-15T18:19:50.095407300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def satisfies_shape(data_graph, shacl_graph, shape, focus):\n",
    "    conforms, g, report = validate(data_graph, shacl_graph=shacl_graph, validate_shapes=[shape], focus=focus)\n",
    "    return conforms\n",
    "\n",
    "\n",
    "def get_shape_target_class(ontology, shape):\n",
    "    return ontology.query(f\"\"\"\n",
    "        PREFIX sh: <{SH}>\n",
    "        SELECT ?targetClass\n",
    "        WHERE {{\n",
    "            <{shape}> sh:targetClass ?targetClass .\n",
    "        }}\n",
    "    \"\"\").bindings[0]['targetClass']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.154403400Z",
     "start_time": "2023-07-15T18:19:50.111403900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_implementation_parameters(ontology, implementation) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value ?order\n",
    "        WHERE {{\n",
    "            <{implementation}> dtbox:hasParameter ?parameter .\n",
    "            ?parameter dtbox:hasDefaultValue ?value ;\n",
    "                       dtbox:has_position ?order .\n",
    "        }}\n",
    "        ORDER BY ?order\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['order']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_overriden_parameters(ontology, component) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value ?position\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:overridesParameter ?parameterValue .\n",
    "            ?parameterValue dtbox:forParameter ?parameter ;\n",
    "                       dtbox:has_value ?value .\n",
    "            ?parameter dtbox:has_position ?position .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['position']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_parameters(ontology, component) -> Dict[URIRef, Tuple[Any, int]]:\n",
    "    implementation = get_component_implementation(ontology, component)\n",
    "    implementation_params = get_implementation_parameters(ontology, implementation)\n",
    "    component_params = get_component_overriden_parameters(ontology, component)\n",
    "    implementation_params.update(component_params)\n",
    "    return implementation_params\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.164404100Z",
     "start_time": "2023-07-15T18:19:50.129404900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def add_step(graph, pipeline, task_name, implementation, parameters, order, previous_task=None, inputs=None,\n",
    "             outputs=None):\n",
    "    if outputs is None:\n",
    "        outputs = []\n",
    "    if inputs is None:\n",
    "        inputs = []\n",
    "    step = dw.term(task_name)\n",
    "    graph.add((pipeline, dtbox.hasStep, step))\n",
    "    graph.add((step, RDF.type, dtbox.Step))\n",
    "    graph.add((step, dtbox.runs, implementation))\n",
    "    graph.add((step, dtbox.has_position, Literal(order)))\n",
    "    for i, input in enumerate(inputs):\n",
    "        in_node = BNode()\n",
    "        graph.add((in_node, RDF.type, dtbox.IO))\n",
    "        graph.add((in_node, dtbox.hasData, input))\n",
    "        graph.add((in_node, dtbox.has_position, Literal(i)))\n",
    "        graph.add((step, dtbox.hasInput, in_node))\n",
    "    for o, output in enumerate(outputs):\n",
    "        out_node = BNode()\n",
    "        graph.add((out_node, RDF.type, dtbox.IO))\n",
    "        graph.add((out_node, dtbox.hasData, output))\n",
    "        graph.add((out_node, dtbox.has_position, Literal(o)))\n",
    "        graph.add((step, dtbox.hasOutput, out_node))\n",
    "    for parameter, (value, _) in parameters.items():\n",
    "        param_value = BNode()\n",
    "        graph.add((step, dtbox.hasParameterValue, param_value))\n",
    "        graph.add((param_value, dtbox.forParameter, parameter))\n",
    "        graph.add((param_value, dtbox.has_value, value))\n",
    "    if previous_task:\n",
    "        if isinstance(previous_task, list):\n",
    "            for previous in previous_task:\n",
    "                graph.add((previous, dtbox.followedBy, step))\n",
    "        else:\n",
    "            graph.add((previous_task, dtbox.followedBy, step))\n",
    "    return step"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.165405Z",
     "start_time": "2023-07-15T18:19:50.154403400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_component_transformations(ontology, component) -> List:\n",
    "    transformation_query = f'''\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?transformation\n",
    "        WHERE {{\n",
    "            <{component}> dtbox:hasTransformation ?transformation_list .\n",
    "            ?transformation_list rdf:rest*/rdf:first ?transformation .\n",
    "        }}\n",
    "    '''\n",
    "    transformations = ontology.query(transformation_query).bindings\n",
    "    return [x['transformation'] for x in transformations]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.172404300Z",
     "start_time": "2023-07-15T18:19:50.158408Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def copy_subgraph(source_graph: Graph, source_node: URIRef, destination_graph: Graph, destination_node: URIRef,\n",
    "                  replace_nodes: bool = True):\n",
    "    visited_nodes = set()\n",
    "    nodes_to_visit = [source_node]\n",
    "    mappings = {source_node: destination_node}\n",
    "\n",
    "    while nodes_to_visit:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        visited_nodes.add(current_node)\n",
    "        for predicate, object in source_graph.predicate_objects(current_node):\n",
    "            if predicate == OWL.sameAs:\n",
    "                continue\n",
    "            if replace_nodes and isinstance(object, IdentifiedNode):\n",
    "                if predicate == RDF.type or object in dmop:\n",
    "                    mappings[object] = object\n",
    "                else:\n",
    "                    if object not in visited_nodes:\n",
    "                        nodes_to_visit.append(object)\n",
    "                    if object not in mappings:\n",
    "                        mappings[object] = BNode()\n",
    "                destination_graph.add((mappings[current_node], predicate, mappings[object]))\n",
    "            else:\n",
    "                destination_graph.add((mappings[current_node], predicate, object))\n",
    "\n",
    "\n",
    "def annotate_io_with_spec(ontology: Graph, workflow_graph: Graph, io: URIRef, io_spec: List[URIRef]):\n",
    "    for spec in io_spec:\n",
    "        io_spec_class = next(ontology.objects(spec, SH.targetClass, True), None)\n",
    "        if io_spec_class is None or (io, RDF.type, io_spec_class) in workflow_graph:\n",
    "            continue\n",
    "        workflow_graph.add((io, RDF.type, io_spec_class))\n",
    "\n",
    "\n",
    "def annotate_ios_with_specs(ontology: Graph, workflow_graph: Graph, io: List[URIRef], specs: List[List[URIRef]]):\n",
    "    assert len(io) == len(specs), 'Number of IOs and specs must be the same'\n",
    "    for io, spec in zip(io, specs):\n",
    "        annotate_io_with_spec(ontology, workflow_graph, io, spec)\n",
    "\n",
    "\n",
    "def run_copy_transformation(ontology: Graph, workflow_graph: Graph, transformation, inputs, outputs):\n",
    "    input_index = next(ontology.objects(transformation, dtbox.copy_input, True)).value\n",
    "    output_index = next(ontology.objects(transformation, dtbox.copy_output, True)).value\n",
    "    input = inputs[input_index - 1]\n",
    "    output = outputs[output_index - 1]\n",
    "\n",
    "    copy_subgraph(workflow_graph, input, workflow_graph, output)\n",
    "\n",
    "\n",
    "def run_component_transformation(ontology: Graph, workflow_graph: Graph, component, inputs, outputs,\n",
    "                                 parameters: dict):\n",
    "    transformations = get_component_transformations(ontology, component)\n",
    "    for transformation in transformations:\n",
    "        if (transformation, RDF.type, dtbox.CopyTransformation) in ontology:\n",
    "            run_copy_transformation(ontology, workflow_graph, transformation, inputs, outputs)\n",
    "        else:\n",
    "            prefixes = f'''\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "PREFIX da: <{da}>\n",
    "PREFIX rdf: <{RDF}>\n",
    "PREFIX rdfs: <{RDFS}>\n",
    "PREFIX owl: <{OWL}>\n",
    "PREFIX xsd: <{XSD}>\n",
    "PREFIX dmop: <{dmop}>\n",
    "'''\n",
    "            query = next(ontology.objects(transformation, dtbox.transformation_query, True)).value\n",
    "            query = prefixes + query\n",
    "            for i in range(len(inputs)):\n",
    "                query = query.replace(f'$input{i + 1}', f'{inputs[i].n3()}')\n",
    "            for i in range(len(outputs)):\n",
    "                query = query.replace(f'$output{i + 1}', f'{outputs[i].n3()}')\n",
    "            for param, (value, order) in parameters.items():\n",
    "                query = query.replace(f'$param{order + 1}', f'{value.toPython()}')\n",
    "                query = query.replace(f'$parameter{order + 1}', f'{value.toPython()}')\n",
    "            workflow_graph.update(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.203403800Z",
     "start_time": "2023-07-15T18:19:50.184405700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def step_name(workflow_name, task_order, implementation):\n",
    "    return f'{workflow_name}-step_{task_order}_{implementation.fragment.replace(\"-\", \"_\")}'\n",
    "\n",
    "\n",
    "def build_workflow_train_test(workflow_name, ontology, dataset, main_component, split_component, transformations):\n",
    "    workflow_graph = get_graph()\n",
    "    workflow = dw.term(workflow_name)\n",
    "    workflow_graph.add((workflow, RDF.type, dtbox.Workflow))\n",
    "    task_order = 0\n",
    "\n",
    "    dataset_node = dw.term(f'{workflow_name}-original_dataset')\n",
    "\n",
    "    copy_subgraph(ontology, dataset, workflow_graph, dataset_node)\n",
    "\n",
    "    split_step_name = step_name(workflow_name, task_order, split_component)\n",
    "    split_outputs = [dw[f'{split_step_name}-output_train'], dw[f'{split_step_name}-output_test']]\n",
    "    split_parameters = get_component_parameters(ontology, split_component)\n",
    "    split_step = add_step(workflow_graph, workflow,\n",
    "                          split_step_name,\n",
    "                          split_component,\n",
    "                          split_parameters,\n",
    "                          task_order,\n",
    "                          None,\n",
    "                          [dataset_node],\n",
    "                          split_outputs)\n",
    "    run_component_transformation(ontology, workflow_graph, split_component,\n",
    "                                 [dataset_node], split_outputs,\n",
    "                                 split_parameters)\n",
    "\n",
    "    task_order += 1\n",
    "\n",
    "    train_dataset_node = split_outputs[0]\n",
    "    test_dataset_node = split_outputs[1]\n",
    "\n",
    "    previous_train_step = split_step\n",
    "    previous_test_step = split_step\n",
    "\n",
    "    for train_component in [*transformations, main_component]:\n",
    "        test_component = next(ontology.objects(train_component, dtbox.hasApplier, True), train_component)\n",
    "        same = train_component == test_component\n",
    "\n",
    "        train_step_name = step_name(workflow_name, task_order, train_component)\n",
    "        test_step_name = step_name(workflow_name, task_order + 1, test_component)\n",
    "\n",
    "        train_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                           get_component_implementation(ontology, train_component))\n",
    "        train_input_data_index = identify_data_io(ontology, train_input_specs, return_index=True)\n",
    "        train_transformation_inputs = [dw[f'{train_step_name}-input_{i}'] for i in range(len(train_input_specs))]\n",
    "        train_transformation_inputs[train_input_data_index] = train_dataset_node\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_inputs,\n",
    "                                train_input_specs)\n",
    "\n",
    "        train_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                             get_component_implementation(ontology, train_component))\n",
    "        train_output_model_index = identify_model_io(ontology, train_output_specs, return_index=True)\n",
    "        train_output_data_index = identify_data_io(ontology, train_output_specs, return_index=True)\n",
    "        train_transformation_outputs = [dw[f'{train_step_name}-output_{i}'] for i in range(len(train_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_outputs,\n",
    "                                train_output_specs)\n",
    "\n",
    "        train_parameters = get_component_parameters(ontology, train_component)\n",
    "        train_step = add_step(workflow_graph, workflow,\n",
    "                              train_step_name,\n",
    "                              train_component, train_parameters, task_order, previous_train_step,\n",
    "                              train_transformation_inputs,\n",
    "                              train_transformation_outputs)\n",
    "\n",
    "        previous_train_step = train_step\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, train_component, train_transformation_inputs,\n",
    "                                     train_transformation_outputs, {})\n",
    "\n",
    "        if train_output_data_index is not None:\n",
    "            train_dataset_node = train_transformation_outputs[train_output_data_index]\n",
    "\n",
    "        task_order += 1\n",
    "\n",
    "        test_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                          get_component_implementation(ontology, test_component))\n",
    "        test_input_data_index = identify_data_io(ontology, test_input_specs, return_index=True)\n",
    "        test_input_model_index = identify_model_io(ontology, test_input_specs, return_index=True)\n",
    "        test_transformation_inputs = [dw[f'{test_step_name}-input_{i}'] for i in range(len(test_input_specs))]\n",
    "        test_transformation_inputs[test_input_data_index] = test_dataset_node\n",
    "        test_transformation_inputs[test_input_model_index] = train_transformation_outputs[train_output_model_index]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_inputs,\n",
    "                                test_input_specs)\n",
    "\n",
    "        test_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                            get_component_implementation(ontology, test_component))\n",
    "        test_output_data_index = identify_data_io(ontology, test_output_specs, return_index=True)\n",
    "        test_transformation_outputs = [dw[f'{test_step_name}-output_{i}'] for i in range(len(test_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_outputs,\n",
    "                                test_output_specs)\n",
    "\n",
    "        previous_test_steps = [previous_test_step, train_step] if not same else [previous_test_step]\n",
    "        test_parameters = get_component_parameters(ontology, test_component)\n",
    "        test_step = add_step(workflow_graph, workflow,\n",
    "                             test_step_name,\n",
    "                             test_component, test_parameters, task_order, previous_test_steps,\n",
    "                             test_transformation_inputs,\n",
    "                             test_transformation_outputs)\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, test_component, test_transformation_inputs,\n",
    "                                     test_transformation_outputs, {})\n",
    "\n",
    "        test_dataset_node = test_transformation_outputs[test_output_data_index]\n",
    "        previous_test_step = test_step\n",
    "        task_order += 1\n",
    "\n",
    "    return workflow_graph, workflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.238404600Z",
     "start_time": "2023-07-15T18:19:50.216404200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=N52393dfdfe994eedb5ed4f6f8ab6403e (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_graph = get_graph()\n",
    "ins = Namespace('https://diviloper.dev/intent#')\n",
    "intent_graph.add((ins.DescriptionIntent, RDF.type, dtbox.Intent))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.overData, dd.term('penguins.csv')))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.tackles, dabox.Description))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.250404500Z",
     "start_time": "2023-07-15T18:19:50.237403900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "log = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:50.294404400Z",
     "start_time": "2023-07-15T18:19:50.251404300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: penguins.csv\n",
      "Problem: Description\n",
      "Intent params: []\n",
      "-------------------------------------------------\n",
      "Component: component-decision_tree_learner (implementation-decision_tree_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape']\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "-------------------------------------------------\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n",
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "Expected end of text, found 'INSERT'  (at char 370), (line:10, col:1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParseException\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 64\u001B[0m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     61\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mCombination \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(transformation_combinations)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[x\u001B[38;5;241m.\u001B[39mfragment \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m transformation_combination]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     63\u001B[0m workflow_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkflow_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworkflow_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mintent_iri\u001B[38;5;241m.\u001B[39mfragment\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muuid\u001B[38;5;241m.\u001B[39muuid4()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 64\u001B[0m wg, w \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_workflow_train_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworkflow_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43montology\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomponent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformation_combination\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mtransformation_combination\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m log:\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mWorkflow \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworkflow_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mw\u001B[38;5;241m.\u001B[39mfragment\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[14], line 103\u001B[0m, in \u001B[0;36mbuild_workflow_train_test\u001B[1;34m(workflow_name, ontology, dataset, main_component, split_component, transformations)\u001B[0m\n\u001B[0;32m     96\u001B[0m test_parameters \u001B[38;5;241m=\u001B[39m get_component_parameters(ontology, test_component)\n\u001B[0;32m     97\u001B[0m test_step \u001B[38;5;241m=\u001B[39m add_step(workflow_graph, workflow,\n\u001B[0;32m     98\u001B[0m                      test_step_name,\n\u001B[0;32m     99\u001B[0m                      test_component, test_parameters, task_order, previous_test_steps,\n\u001B[0;32m    100\u001B[0m                      test_transformation_inputs,\n\u001B[0;32m    101\u001B[0m                      test_transformation_outputs)\n\u001B[1;32m--> 103\u001B[0m \u001B[43mrun_component_transformation\u001B[49m\u001B[43m(\u001B[49m\u001B[43montology\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworkflow_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_component\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_transformation_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mtest_transformation_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    106\u001B[0m test_dataset_node \u001B[38;5;241m=\u001B[39m test_transformation_outputs[test_output_data_index]\n\u001B[0;32m    107\u001B[0m previous_test_step \u001B[38;5;241m=\u001B[39m test_step\n",
      "Cell \u001B[1;32mIn[13], line 74\u001B[0m, in \u001B[0;36mrun_component_transformation\u001B[1;34m(ontology, workflow_graph, component, inputs, outputs, parameters)\u001B[0m\n\u001B[0;32m     72\u001B[0m     query \u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$param\u001B[39m\u001B[38;5;132;01m{\u001B[39;00morder \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;241m.\u001B[39mtoPython()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     73\u001B[0m     query \u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$parameter\u001B[39m\u001B[38;5;132;01m{\u001B[39;00morder \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;241m.\u001B[39mtoPython()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 74\u001B[0m \u001B[43mworkflow_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Ontology\\lib\\site-packages\\rdflib\\graph.py:1612\u001B[0m, in \u001B[0;36mGraph.update\u001B[1;34m(self, update_object, processor, initNs, initBindings, use_store_provided, **kwargs)\u001B[0m\n\u001B[0;32m   1609\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processor, query\u001B[38;5;241m.\u001B[39mUpdateProcessor):\n\u001B[0;32m   1610\u001B[0m     processor \u001B[38;5;241m=\u001B[39m plugin\u001B[38;5;241m.\u001B[39mget(processor, query\u001B[38;5;241m.\u001B[39mUpdateProcessor)(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m-> 1612\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mprocessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mupdate_object\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitBindings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitNs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Ontology\\lib\\site-packages\\rdflib\\plugins\\sparql\\processor.py:95\u001B[0m, in \u001B[0;36mSPARQLUpdateProcessor.update\u001B[1;34m(self, strOrQuery, initBindings, initNs)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124;03m.. caution::\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;124;03m   documentation.\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(strOrQuery, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m---> 95\u001B[0m     strOrQuery \u001B[38;5;241m=\u001B[39m translateUpdate(\u001B[43mparseUpdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstrOrQuery\u001B[49m\u001B[43m)\u001B[49m, initNs\u001B[38;5;241m=\u001B[39minitNs)\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m evalUpdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph, strOrQuery, initBindings)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Ontology\\lib\\site-packages\\rdflib\\plugins\\sparql\\parser.py:1553\u001B[0m, in \u001B[0;36mparseUpdate\u001B[1;34m(q)\u001B[0m\n\u001B[0;32m   1550\u001B[0m     q \u001B[38;5;241m=\u001B[39m q\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1552\u001B[0m q \u001B[38;5;241m=\u001B[39m expandUnicodeEscapes(q)\n\u001B[1;32m-> 1553\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mUpdateUnit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparseString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparseAll\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Ontology\\lib\\site-packages\\pyparsing\\core.py:1141\u001B[0m, in \u001B[0;36mParserElement.parse_string\u001B[1;34m(self, instring, parse_all, parseAll)\u001B[0m\n\u001B[0;32m   1138\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m   1139\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1140\u001B[0m         \u001B[38;5;66;03m# catch and re-raise exception from here, clearing out pyparsing internal stack trace\u001B[39;00m\n\u001B[1;32m-> 1141\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exc\u001B[38;5;241m.\u001B[39mwith_traceback(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m   1142\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokens\n",
      "\u001B[1;31mParseException\u001B[0m: Expected end of text, found 'INSERT'  (at char 370), (line:10, col:1)"
     ]
    }
   ],
   "source": [
    "dataset, problem, intent_params, intent_iri = get_intent_info(intent_graph)\n",
    "\n",
    "if log:\n",
    "    print(f'Dataset: {dataset.fragment}')\n",
    "    print(f'Problem: {problem.fragment}')\n",
    "    print(f'Intent params: {intent_params}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "comps = get_potential_implementations(ontology, problem, [x['param'] for x in intent_params])\n",
    "components = [\n",
    "    (c, impl, inputs)\n",
    "    for impl, inputs in comps\n",
    "    for c in get_implementation_components(ontology, impl)\n",
    "]\n",
    "if log:\n",
    "    for component, implementation, inputs in components:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "        for im_input in inputs:\n",
    "            print(f'\\tInput: {[x.fragment for x in im_input]}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "workflow_order = 0\n",
    "\n",
    "split_components = [\n",
    "    dabox.term('component-random_absolute_train_test_split'),\n",
    "    dabox.term('component-random_relative_train_test_split'),\n",
    "    dabox.term('component-top_k_absolute_train_test_split'),\n",
    "    dabox.term('component-top_k_relative_train_test_split'),\n",
    "]\n",
    "\n",
    "for component, implementation, inputs in components[1:]:\n",
    "    if log:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "    shapes_to_satisfy = identify_data_io(ontology, inputs)\n",
    "    assert shapes_to_satisfy is not None and len(shapes_to_satisfy) > 0\n",
    "    if log:\n",
    "        print(f'\\tData input: {[x.fragment for x in shapes_to_satisfy]}')\n",
    "\n",
    "    unsatisfied_shapes = [shape for shape in shapes_to_satisfy if\n",
    "                          not satisfies_shape(ontology, ontology, shape, dataset)]\n",
    "\n",
    "    available_transformations = {\n",
    "        shape: find_components_to_satisfy_shape(ontology, shape, only_learners=True)\n",
    "        for shape in unsatisfied_shapes\n",
    "    }\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tUnsatisfied shapes: ')\n",
    "        for shape, comps in available_transformations.items():\n",
    "            print(f'\\t\\t{shape.fragment}: {[x.fragment for x in comps]}')\n",
    "\n",
    "    transformation_combinations = list(itertools.product(split_components, *available_transformations.values()))\n",
    "    # TODO - check if the combination is valid and whether further transformations are needed\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tTotal combinations: {len(transformation_combinations)}')\n",
    "\n",
    "    for i, transformation_combination in enumerate(transformation_combinations):\n",
    "        if log:\n",
    "            print(\n",
    "                f'\\t\\tCombination {i + 1} / {len(transformation_combinations)}: {[x.fragment for x in transformation_combination]}')\n",
    "\n",
    "        workflow_name = f'workflow_{workflow_order}_{intent_iri.fragment}_{uuid.uuid4()}'.replace('-', '_')\n",
    "        wg, w = build_workflow_train_test(workflow_name, ontology, dataset, component, transformation_combination[0],\n",
    "                                          transformation_combination[1:])\n",
    "        if log:\n",
    "            print(f'\\t\\tWorkflow {workflow_order}: {w.fragment}')\n",
    "        wg.serialize(f'{workflow_name}.ttl', format='turtle')\n",
    "        workflow_order += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T18:19:53.120425700Z",
     "start_time": "2023-07-15T18:19:50.273404500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
