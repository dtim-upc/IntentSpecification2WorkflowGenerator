{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:36.368712700Z",
     "start_time": "2023-07-15T19:40:36.265418100Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Any, List, Dict\n",
    "\n",
    "from pyshacl import validate\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline generation algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ontology = get_ontology_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:37.958697700Z",
     "start_time": "2023-07-15T19:40:36.369711200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Obtain Intent Information functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:50.458486Z",
     "start_time": "2023-07-15T13:00:50.270487Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_intent_iri(intent_graph):\n",
    "    intent_iri_query = f\"\"\"\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "SELECT ?iri\n",
    "WHERE {{\n",
    "    ?iri a dtbox:Intent .\n",
    "}}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(intent_iri_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['iri']\n",
    "\n",
    "\n",
    "def get_intent_dataset_problem(intent_graph, intent_iri):\n",
    "    dataset_problem_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?dataset ?problem\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:Intent .\n",
    "        {intent_iri.n3()} dtbox:overData ?dataset .\n",
    "        {intent_iri.n3()} dtbox:tackles ?problem .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(dataset_problem_query).bindings[0]\n",
    "    return result['dataset'], result['problem']\n",
    "\n",
    "\n",
    "def get_intent_params(intent_graph, intent_iri):\n",
    "    params_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?param ?value\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:UserIntent .\n",
    "        {intent_iri.n3()} dtbox:usingParameter ?param_value .\n",
    "        ?param_value dtbox:forParameter ?param .\n",
    "        ?param_value dtbox:has_value ?value .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(params_query).bindings\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_intent_info(intent_graph, intent_iri=None) -> Tuple[Any, Any, List[Any], Any]:\n",
    "    if not intent_iri:\n",
    "        intent_iri = get_intent_iri(intent_graph)\n",
    "\n",
    "    dataset, problem = get_intent_dataset_problem(intent_graph, intent_iri)\n",
    "    params = get_intent_params(intent_graph, intent_iri)\n",
    "\n",
    "    return dataset, problem, params, intent_iri"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:37.976696500Z",
     "start_time": "2023-07-15T19:40:37.953682400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Obtain Loader functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:55.379704800Z",
     "start_time": "2023-07-15T13:00:55.359705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nee3a405cc25d4048bd7801084c90ab48b101 http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://diviloper.dev/ontology#IOSpec\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#hasTag https://diviloper.dev/ontology/shapes#LabeledTabularDatasetShape\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#has_position 0\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:37.991695700Z",
     "start_time": "2023-07-15T19:40:37.967696100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Obtain Main component dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_implementation_input_specs(ontology, implementation):\n",
    "    input_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesInput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(input_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def get_implementation_output_specs(ontology, implementation):\n",
    "    output_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesOutput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(output_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def flatten_shape(graph, shape):\n",
    "    if (shape, SH['and'], None) in graph:\n",
    "        subshapes_query = f\"\"\"\n",
    "            PREFIX sh: <{SH}>\n",
    "            PREFIX rdf: <{RDF}>\n",
    "\n",
    "            SELECT ?subshape\n",
    "            WHERE {{\n",
    "                {shape.n3()} sh:and ?andNode .\n",
    "                ?andNode rdf:rest*/rdf:first ?subshape .\n",
    "            }}\n",
    "        \"\"\"\n",
    "        subshapes = graph.query(subshapes_query).bindings\n",
    "\n",
    "        return [x for subshape in subshapes for x in flatten_shape(graph, subshape['subshape'])]\n",
    "    else:\n",
    "        return [shape]\n",
    "\n",
    "\n",
    "def get_potential_implementations(ontology, problem_iri, intent_parameters=None) -> List[Tuple[Any, List[Any]]]:\n",
    "    if intent_parameters is None:\n",
    "        intent_parameters = []\n",
    "    intent_params_match = [f'dtbox:hasParameter {param.n3()} ;' for param in intent_parameters]\n",
    "    intent_params_separator = '            \\n'\n",
    "    main_implementation_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?implementation\n",
    "    WHERE {{\n",
    "        ?implementation a dtbox:Implementation ;\n",
    "            {intent_params_separator.join(intent_params_match)}\n",
    "            dtbox:implements ?algorithm .\n",
    "        ?algorithm a dtbox:Algorithm ;\n",
    "            dtbox:solves ?problem .\n",
    "        ?problem dtbox:subProblemOf* {problem_iri.n3()} .\n",
    "        FILTER NOT EXISTS{{\n",
    "            ?implementation a dtbox:ApplierImplementation.\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "    results = ontology.query(main_implementation_query).bindings\n",
    "    implementations = [result['implementation'] for result in results]\n",
    "\n",
    "    implementations_with_shapes = [\n",
    "        (implementation, get_implementation_input_specs(ontology, implementation))\n",
    "        for implementation in implementations]\n",
    "\n",
    "    return implementations_with_shapes\n",
    "\n",
    "\n",
    "def get_component_implementation(ontology, component):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:hasImplementation ?implementation .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['implementation']\n",
    "\n",
    "\n",
    "def get_implementation_components(ontology, implementation) -> List[Any]:\n",
    "    components_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?component\n",
    "        WHERE {{\n",
    "            ?component dtbox:hasImplementation {implementation.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(components_query).bindings\n",
    "    return [result['component'] for result in results]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.007748600Z",
     "start_time": "2023-07-15T19:40:37.993698400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def find_components_to_satisfy_shape(ontology, shape, only_learners=True):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            ?implementation a dtbox:{'Learner' if only_learners else ''}Implementation ;\n",
    "                dtbox:specifiesOutput ?spec .\n",
    "            ?spec dtbox:hasTag {shape.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    implementations = [x['implementation'] for x in result]\n",
    "    components = [c\n",
    "                  for implementation in implementations\n",
    "                  for c in get_implementation_components(ontology, implementation)]\n",
    "    return components"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.021748400Z",
     "start_time": "2023-07-15T19:40:38.000748400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def identify_data_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            if (io_shape, SH.targetClass, dmop.TabularDataset) in ontology:\n",
    "                return i if return_index else io_shapes\n",
    "\n",
    "\n",
    "def identify_model_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            query = f'''\n",
    "    PREFIX sh: <{SH}>\n",
    "    PREFIX rdfs: <{RDFS}>\n",
    "    PREFIX ddata: <{dd}>\n",
    "\n",
    "    ASK {{\n",
    "      {{\n",
    "        {io_shape.n3()} sh:targetClass ?targetClass .\n",
    "        ?targetClass rdfs:subClassOf* ddata:Model .\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        {io_shape.n3()} rdfs:subClassOf* ddata:Model .\n",
    "      }}\n",
    "    }}\n",
    "'''\n",
    "            if ontology.query(query).askAnswer:\n",
    "                return i if return_index else io_shapes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.028748800Z",
     "start_time": "2023-07-15T19:40:38.018749800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: https://diviloper.dev/ontology/ABOX#component-decimal_scaling\n",
      "Implementation: https://diviloper.dev/ontology/ABOX#implementation-normalizer_(pmml)\n",
      "Specs: [[rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizedTabularDatasetShape')], [rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizerModel')]]\n",
      "Model spec: [rdflib.term.URIRef('https://diviloper.dev/ontology/shapes#NormalizerModel')]\n"
     ]
    }
   ],
   "source": [
    "comp = dabox.term('component-decimal_scaling')\n",
    "print(f'Component: {comp}')\n",
    "impl = get_component_implementation(ontology, comp)\n",
    "print(f'Implementation: {impl}')\n",
    "specs = get_implementation_output_specs(ontology, impl)\n",
    "print(f'Specs: {specs}')\n",
    "model_spec = identify_model_io(ontology, specs)\n",
    "print(f'Model spec: {model_spec}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.201840200Z",
     "start_time": "2023-07-15T19:40:38.030748700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/ns/shacl#NodeShape\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://diviloper.dev/ontology#DataTag\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2002/07/owl#Thing\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/ns/shacl#targetClass https://diviloper.dev/ontology/Data#NormalizerModel\n",
      "https://diviloper.dev/ontology/shapes#NormalizerModel http://www.w3.org/2002/07/owl#sameAs https://diviloper.dev/ontology/shapes#NormalizerModel\n"
     ]
    }
   ],
   "source": [
    "for s, p, o in ontology.triples((specs[1][0], None, None)):\n",
    "    print(f'{s} {p} {o}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.217841100Z",
     "start_time": "2023-07-15T19:40:38.204841800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def satisfies_shape(data_graph, shacl_graph, shape, focus):\n",
    "    conforms, g, report = validate(data_graph, shacl_graph=shacl_graph, validate_shapes=[shape], focus=focus)\n",
    "    return conforms\n",
    "\n",
    "\n",
    "def get_shape_target_class(ontology, shape):\n",
    "    return ontology.query(f\"\"\"\n",
    "        PREFIX sh: <{SH}>\n",
    "        SELECT ?targetClass\n",
    "        WHERE {{\n",
    "            <{shape}> sh:targetClass ?targetClass .\n",
    "        }}\n",
    "    \"\"\").bindings[0]['targetClass']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.261897Z",
     "start_time": "2023-07-15T19:40:38.220840900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_implementation_parameters(ontology, implementation) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value ?order\n",
    "        WHERE {{\n",
    "            <{implementation}> dtbox:hasParameter ?parameter .\n",
    "            ?parameter dtbox:hasDefaultValue ?value ;\n",
    "                       dtbox:has_position ?order .\n",
    "        }}\n",
    "        ORDER BY ?order\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['order']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_overriden_parameters(ontology, component) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value ?position\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:overridesParameter ?parameterValue .\n",
    "            ?parameterValue dtbox:forParameter ?parameter ;\n",
    "                       dtbox:has_value ?value .\n",
    "            ?parameter dtbox:has_position ?position .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: (param['value'], param['position']) for param in results}\n",
    "\n",
    "\n",
    "def get_component_parameters(ontology, component) -> Dict[URIRef, Tuple[Any, int]]:\n",
    "    implementation = get_component_implementation(ontology, component)\n",
    "    implementation_params = get_implementation_parameters(ontology, implementation)\n",
    "    component_params = get_component_overriden_parameters(ontology, component)\n",
    "    implementation_params.update(component_params)\n",
    "    return implementation_params\n",
    "\n",
    "def perform_param_substitution(graph, parameters, inputs):\n",
    "    for param in parameters.keys():\n",
    "        value, order = parameters[param]\n",
    "        if isinstance(value.value, str) and '$$LABEL$$' in value.value:\n",
    "            new_value = value.replace('$$LABEL$$', f'{get_inputs_label_name(graph, inputs)}')\n",
    "            parameters[param] = (Literal(new_value), order)\n",
    "    return parameters\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.278897600Z",
     "start_time": "2023-07-15T19:40:38.239884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def add_step(graph, pipeline, task_name, implementation, parameters, order, previous_task=None, inputs=None,\n",
    "             outputs=None):\n",
    "    if outputs is None:\n",
    "        outputs = []\n",
    "    if inputs is None:\n",
    "        inputs = []\n",
    "    step = dw.term(task_name)\n",
    "    graph.add((pipeline, dtbox.hasStep, step))\n",
    "    graph.add((step, RDF.type, dtbox.Step))\n",
    "    graph.add((step, dtbox.runs, implementation))\n",
    "    graph.add((step, dtbox.has_position, Literal(order)))\n",
    "    for i, input in enumerate(inputs):\n",
    "        in_node = BNode()\n",
    "        graph.add((in_node, RDF.type, dtbox.IO))\n",
    "        graph.add((in_node, dtbox.hasData, input))\n",
    "        graph.add((in_node, dtbox.has_position, Literal(i)))\n",
    "        graph.add((step, dtbox.hasInput, in_node))\n",
    "    for o, output in enumerate(outputs):\n",
    "        out_node = BNode()\n",
    "        graph.add((out_node, RDF.type, dtbox.IO))\n",
    "        graph.add((out_node, dtbox.hasData, output))\n",
    "        graph.add((out_node, dtbox.has_position, Literal(o)))\n",
    "        graph.add((step, dtbox.hasOutput, out_node))\n",
    "    for parameter, (value, _) in parameters.items():\n",
    "        param_value = BNode()\n",
    "        graph.add((step, dtbox.hasParameterValue, param_value))\n",
    "        graph.add((param_value, dtbox.forParameter, parameter))\n",
    "        graph.add((param_value, dtbox.has_value, value))\n",
    "    if previous_task:\n",
    "        if isinstance(previous_task, list):\n",
    "            for previous in previous_task:\n",
    "                graph.add((previous, dtbox.followedBy, step))\n",
    "        else:\n",
    "            graph.add((previous_task, dtbox.followedBy, step))\n",
    "    return step"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.279897300Z",
     "start_time": "2023-07-15T19:40:38.254903300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_component_transformations(ontology, component) -> List:\n",
    "    transformation_query = f'''\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?transformation\n",
    "        WHERE {{\n",
    "            <{component}> dtbox:hasTransformation ?transformation_list .\n",
    "            ?transformation_list rdf:rest*/rdf:first ?transformation .\n",
    "        }}\n",
    "    '''\n",
    "    transformations = ontology.query(transformation_query).bindings\n",
    "    return [x['transformation'] for x in transformations]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.279897300Z",
     "start_time": "2023-07-15T19:40:38.266898400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_inputs_label_name(graph: Graph, inputs: List[URIRef]) -> str:\n",
    "    data_input = next(i for i in inputs if (i, RDF.type, dmop.TabularDataset) in graph)\n",
    "    label_query = f\"\"\"\n",
    "        PREFIX rdfs: <{RDFS}>\n",
    "        PREFIX dmop: <{dmop}>\n",
    "\n",
    "        SELECT ?label\n",
    "        WHERE {{\n",
    "            {data_input.n3()} dmop:hasColumn ?column .\n",
    "            ?column dmop:isLabel true ;\n",
    "                    dmop:hasColumnName ?label .\n",
    "\n",
    "        }}\n",
    "    \"\"\"\n",
    "    return graph.query(label_query).bindings[0]['label'].value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.302897100Z",
     "start_time": "2023-07-15T19:40:38.283897400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def copy_subgraph(source_graph: Graph, source_node: URIRef, destination_graph: Graph, destination_node: URIRef,\n",
    "                  replace_nodes: bool = True):\n",
    "    visited_nodes = set()\n",
    "    nodes_to_visit = [source_node]\n",
    "    mappings = {source_node: destination_node}\n",
    "\n",
    "    while nodes_to_visit:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        visited_nodes.add(current_node)\n",
    "        for predicate, object in source_graph.predicate_objects(current_node):\n",
    "            if predicate == OWL.sameAs:\n",
    "                continue\n",
    "            if replace_nodes and isinstance(object, IdentifiedNode):\n",
    "                if predicate == RDF.type or object in dmop:\n",
    "                    mappings[object] = object\n",
    "                else:\n",
    "                    if object not in visited_nodes:\n",
    "                        nodes_to_visit.append(object)\n",
    "                    if object not in mappings:\n",
    "                        mappings[object] = BNode()\n",
    "                destination_graph.add((mappings[current_node], predicate, mappings[object]))\n",
    "            else:\n",
    "                destination_graph.add((mappings[current_node], predicate, object))\n",
    "\n",
    "\n",
    "def annotate_io_with_spec(ontology: Graph, workflow_graph: Graph, io: URIRef, io_spec: List[URIRef]):\n",
    "    for spec in io_spec:\n",
    "        io_spec_class = next(ontology.objects(spec, SH.targetClass, True), None)\n",
    "        if io_spec_class is None or (io, RDF.type, io_spec_class) in workflow_graph:\n",
    "            continue\n",
    "        workflow_graph.add((io, RDF.type, io_spec_class))\n",
    "\n",
    "\n",
    "def annotate_ios_with_specs(ontology: Graph, workflow_graph: Graph, io: List[URIRef], specs: List[List[URIRef]]):\n",
    "    assert len(io) == len(specs), 'Number of IOs and specs must be the same'\n",
    "    for io, spec in zip(io, specs):\n",
    "        annotate_io_with_spec(ontology, workflow_graph, io, spec)\n",
    "\n",
    "\n",
    "def run_copy_transformation(ontology: Graph, workflow_graph: Graph, transformation, inputs, outputs):\n",
    "    input_index = next(ontology.objects(transformation, dtbox.copy_input, True)).value\n",
    "    output_index = next(ontology.objects(transformation, dtbox.copy_output, True)).value\n",
    "    input = inputs[input_index - 1]\n",
    "    output = outputs[output_index - 1]\n",
    "\n",
    "    copy_subgraph(workflow_graph, input, workflow_graph, output)\n",
    "\n",
    "\n",
    "def run_component_transformation(ontology: Graph, workflow_graph: Graph, component, inputs, outputs,\n",
    "                                 parameters: dict):\n",
    "    transformations = get_component_transformations(ontology, component)\n",
    "    for transformation in transformations:\n",
    "        if (transformation, RDF.type, dtbox.CopyTransformation) in ontology:\n",
    "            run_copy_transformation(ontology, workflow_graph, transformation, inputs, outputs)\n",
    "        else:\n",
    "            prefixes = f'''\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "PREFIX da: <{da}>\n",
    "PREFIX rdf: <{RDF}>\n",
    "PREFIX rdfs: <{RDFS}>\n",
    "PREFIX owl: <{OWL}>\n",
    "PREFIX xsd: <{XSD}>\n",
    "PREFIX dmop: <{dmop}>\n",
    "'''\n",
    "            query = next(ontology.objects(transformation, dtbox.transformation_query, True)).value\n",
    "            query = prefixes + query\n",
    "            for i in range(len(inputs)):\n",
    "                query = query.replace(f'$input{i + 1}', f'{inputs[i].n3()}')\n",
    "            for i in range(len(outputs)):\n",
    "                query = query.replace(f'$output{i + 1}', f'{outputs[i].n3()}')\n",
    "            for param, (value, order) in parameters.items():\n",
    "                query = query.replace(f'$param{order + 1}', f'{value.n3()}')\n",
    "                query = query.replace(f'$parameter{order + 1}', f'{value.n3()}')\n",
    "            workflow_graph.update(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.329925400Z",
     "start_time": "2023-07-15T19:40:38.306898100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def step_name(workflow_name, task_order, implementation):\n",
    "    return f'{workflow_name}-step_{task_order}_{implementation.fragment.replace(\"-\", \"_\")}'\n",
    "\n",
    "\n",
    "def build_workflow_train_test(workflow_name, ontology, dataset, main_component, split_component, transformations):\n",
    "    workflow_graph = get_graph()\n",
    "    workflow = dw.term(workflow_name)\n",
    "    workflow_graph.add((workflow, RDF.type, dtbox.Workflow))\n",
    "    task_order = 0\n",
    "\n",
    "    dataset_node = dw.term(f'{workflow_name}-original_dataset')\n",
    "\n",
    "    copy_subgraph(ontology, dataset, workflow_graph, dataset_node)\n",
    "\n",
    "    split_step_name = step_name(workflow_name, task_order, split_component)\n",
    "    split_outputs = [dw[f'{split_step_name}-output_train'], dw[f'{split_step_name}-output_test']]\n",
    "    split_parameters = get_component_parameters(ontology, split_component)\n",
    "    split_step = add_step(workflow_graph, workflow,\n",
    "                          split_step_name,\n",
    "                          split_component,\n",
    "                          split_parameters,\n",
    "                          task_order,\n",
    "                          None,\n",
    "                          [dataset_node],\n",
    "                          split_outputs)\n",
    "    run_component_transformation(ontology, workflow_graph, split_component,\n",
    "                                 [dataset_node], split_outputs,\n",
    "                                 split_parameters)\n",
    "\n",
    "    task_order += 1\n",
    "\n",
    "    train_dataset_node = split_outputs[0]\n",
    "    test_dataset_node = split_outputs[1]\n",
    "\n",
    "    previous_train_step = split_step\n",
    "    previous_test_step = split_step\n",
    "\n",
    "    for train_component in [*transformations, main_component]:\n",
    "        test_component = next(ontology.objects(train_component, dtbox.hasApplier, True), train_component)\n",
    "        same = train_component == test_component\n",
    "\n",
    "        train_step_name = step_name(workflow_name, task_order, train_component)\n",
    "        test_step_name = step_name(workflow_name, task_order + 1, test_component)\n",
    "\n",
    "        train_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                           get_component_implementation(ontology, train_component))\n",
    "        train_input_data_index = identify_data_io(ontology, train_input_specs, return_index=True)\n",
    "        train_transformation_inputs = [dw[f'{train_step_name}-input_{i}'] for i in range(len(train_input_specs))]\n",
    "        train_transformation_inputs[train_input_data_index] = train_dataset_node\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_inputs,\n",
    "                                train_input_specs)\n",
    "\n",
    "        train_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                             get_component_implementation(ontology, train_component))\n",
    "        train_output_model_index = identify_model_io(ontology, train_output_specs, return_index=True)\n",
    "        train_output_data_index = identify_data_io(ontology, train_output_specs, return_index=True)\n",
    "        train_transformation_outputs = [dw[f'{train_step_name}-output_{i}'] for i in range(len(train_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_outputs,\n",
    "                                train_output_specs)\n",
    "\n",
    "        train_parameters = get_component_parameters(ontology, train_component)\n",
    "        train_parameters = perform_param_substitution(workflow_graph, train_parameters, train_transformation_inputs)\n",
    "        train_step = add_step(workflow_graph, workflow,\n",
    "                              train_step_name,\n",
    "                              train_component, train_parameters, task_order, previous_train_step,\n",
    "                              train_transformation_inputs,\n",
    "                              train_transformation_outputs)\n",
    "\n",
    "        previous_train_step = train_step\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, train_component, train_transformation_inputs,\n",
    "                                     train_transformation_outputs, train_parameters)\n",
    "\n",
    "        if train_output_data_index is not None:\n",
    "            train_dataset_node = train_transformation_outputs[train_output_data_index]\n",
    "\n",
    "        task_order += 1\n",
    "\n",
    "        test_input_specs = get_implementation_input_specs(ontology,\n",
    "                                                          get_component_implementation(ontology, test_component))\n",
    "        test_input_data_index = identify_data_io(ontology, test_input_specs, return_index=True)\n",
    "        test_input_model_index = identify_model_io(ontology, test_input_specs, return_index=True)\n",
    "        test_transformation_inputs = [dw[f'{test_step_name}-input_{i}'] for i in range(len(test_input_specs))]\n",
    "        test_transformation_inputs[test_input_data_index] = test_dataset_node\n",
    "        test_transformation_inputs[test_input_model_index] = train_transformation_outputs[train_output_model_index]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_inputs,\n",
    "                                test_input_specs)\n",
    "\n",
    "        test_output_specs = get_implementation_output_specs(ontology,\n",
    "                                                            get_component_implementation(ontology, test_component))\n",
    "        test_output_data_index = identify_data_io(ontology, test_output_specs, return_index=True)\n",
    "        test_transformation_outputs = [dw[f'{test_step_name}-output_{i}'] for i in range(len(test_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_outputs,\n",
    "                                test_output_specs)\n",
    "\n",
    "        previous_test_steps = [previous_test_step, train_step] if not same else [previous_test_step]\n",
    "        test_parameters = get_component_parameters(ontology, test_component)\n",
    "        test_parameters = perform_param_substitution(workflow_graph, test_parameters, test_transformation_inputs)\n",
    "        test_step = add_step(workflow_graph, workflow,\n",
    "                             test_step_name,\n",
    "                             test_component, test_parameters, task_order, previous_test_steps,\n",
    "                             test_transformation_inputs,\n",
    "                             test_transformation_outputs)\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, test_component, test_transformation_inputs,\n",
    "                                     test_transformation_outputs, test_parameters)\n",
    "\n",
    "        test_dataset_node = test_transformation_outputs[test_output_data_index]\n",
    "        previous_test_step = test_step\n",
    "        task_order += 1\n",
    "\n",
    "    return workflow_graph, workflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.362938800Z",
     "start_time": "2023-07-15T19:40:38.340926100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Graph identifier=N1a75c185b520456b8961edb6efea79da (<class 'rdflib.graph.Graph'>)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_graph = get_graph()\n",
    "ins = Namespace('https://diviloper.dev/intent#')\n",
    "intent_graph.add((ins.DescriptionIntent, RDF.type, dtbox.Intent))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.overData, dd.term('penguins.csv')))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.tackles, dabox.Description))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.375951700Z",
     "start_time": "2023-07-15T19:40:38.360938900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "log = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:40:38.418995300Z",
     "start_time": "2023-07-15T19:40:38.374952500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: penguins.csv\n",
      "Problem: Description\n",
      "Intent params: []\n",
      "-------------------------------------------------\n",
      "Component: component-decision_tree_learner (implementation-decision_tree_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape']\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "-------------------------------------------------\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n",
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 0: workflow_0_DescriptionIntent_b949aa3c_e664_43eb_ae32_004cac74fd24\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 1: workflow_1_DescriptionIntent_585063e7_8d3a_4a6b_a483_4348fa8cbde4\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 2: workflow_2_DescriptionIntent_b1fce364_77d4_494b_bed0_86f23cd17177\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 3: workflow_3_DescriptionIntent_12f1f619_5e3a_42b0_8a94_b68f19f767f7\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 4: workflow_4_DescriptionIntent_793f562c_2a31_4844_8cbb_d00d728068cd\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 5: workflow_5_DescriptionIntent_18a24770_219a_4f1a_a304_803dfbbc345a\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 6: workflow_6_DescriptionIntent_0f4f6b84_80a9_42bc_b551_109ed91b9864\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 7: workflow_7_DescriptionIntent_89cbb0af_c258_4951_8bc1_709b885a93d0\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 8: workflow_8_DescriptionIntent_33fabd84_ac19_40d5_ad0e_ead494eee338\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 9: workflow_9_DescriptionIntent_2a68e878_a80f_4f9f_a5a0_692efd1d9269\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 10: workflow_10_DescriptionIntent_e7449a4c_edc4_418d_9612_49f10d567052\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 11: workflow_11_DescriptionIntent_d9ea3577_01a9_44d0_b863_d20131e107f8\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 12: workflow_12_DescriptionIntent_0865656c_97d0_4195_a96d_590afec30383\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 13: workflow_13_DescriptionIntent_5849cc6b_9046_4436_ba86_1231f1c73220\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 14: workflow_14_DescriptionIntent_819b1eda_dc59_43cf_af5c_059b79e7afd6\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 15: workflow_15_DescriptionIntent_436e07e1_d110_4c8d_903e_6e52eee1b71a\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 16: workflow_16_DescriptionIntent_9f467925_7b3d_4a7a_b645_e470fccb7558\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 17: workflow_17_DescriptionIntent_f0a14d30_b2ae_4310_9c20_09bababc59dc\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 18: workflow_18_DescriptionIntent_81433b6b_8a68_41ab_aef6_d15d8074ed32\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 19: workflow_19_DescriptionIntent_65e3e866_da83_4b55_9cb9_087ab2945702\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 20: workflow_20_DescriptionIntent_9b67ea79_9c01_4316_9564_d8bf21f90801\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 21: workflow_21_DescriptionIntent_b4138650_ac2d_4d7d_a52b_36ca7578d88b\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 22: workflow_22_DescriptionIntent_878f737b_5ece_4c8e_9deb_3a8cde4542a6\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 23: workflow_23_DescriptionIntent_79cb6980_bbc0_4abe_bbe1_5ea91577ffb3\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n",
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 24: workflow_24_DescriptionIntent_41c0e2c9_7f4d_489a_8606_ba8429d485f9\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 25: workflow_25_DescriptionIntent_6589bba7_0cfc_44f4_b3eb_d5e2addfba32\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 26: workflow_26_DescriptionIntent_c7ba4eed_bb71_405d_96e2_32560af0c7e3\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 27: workflow_27_DescriptionIntent_82e77f4c_e4df_40a7_bc88_c12764644291\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 28: workflow_28_DescriptionIntent_47fc1f93_436c_4c03_baed_7717dd62d149\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 29: workflow_29_DescriptionIntent_a11b62c0_b5dd_4e47_88c8_6b7592fbd46f\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 30: workflow_30_DescriptionIntent_68bd6df3_8ed7_4d75_b08b_687e819a4993\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 31: workflow_31_DescriptionIntent_715c9652_7916_4862_98bd_886630c33ffe\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 32: workflow_32_DescriptionIntent_94366c00_0e03_4909_88bc_46775b6f9850\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 33: workflow_33_DescriptionIntent_4a53989f_787d_4969_8c68_4f0a74df4a43\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 34: workflow_34_DescriptionIntent_fcb389e0_46fc_4801_92c3_f40f70ae3cb9\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 35: workflow_35_DescriptionIntent_5a7b9333_25d3_4122_a531_6769f121f3da\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 36: workflow_36_DescriptionIntent_827b6ade_4523_40d7_a52c_0c4a3895f482\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 37: workflow_37_DescriptionIntent_23e75a28_c749_4e1a_a13d_8936687e5973\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 38: workflow_38_DescriptionIntent_f6c04850_6887_4813_bb95_4d11e06ad95e\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 39: workflow_39_DescriptionIntent_21d9968d_f96b_401d_a15c_a4c47c56a82e\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 40: workflow_40_DescriptionIntent_d5d1b5b1_0a5f_4306_9cba_692d1663c860\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 41: workflow_41_DescriptionIntent_529f37de_7d9b_47a2_96cb_f2ee59e86220\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 42: workflow_42_DescriptionIntent_fecc3445_eb69_4f0d_85ac_e3afd7a119f7\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 43: workflow_43_DescriptionIntent_b5387b90_3a9b_4dd5_9e56_de26b6d40b8a\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 44: workflow_44_DescriptionIntent_acfe139a_f33f_4ae2_8427_a38be696e9cb\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 45: workflow_45_DescriptionIntent_280d2570_6842_4716_8087_9c41a9521582\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 46: workflow_46_DescriptionIntent_0c18d558_36e0_46b9_8feb_c94f2184bac4\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 47: workflow_47_DescriptionIntent_c28a1479_0a00_4646_b65b_480ed4ca8b12\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tData input: ['NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape', 'LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape', 'NonNullTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\t\tNormalizedTabularDatasetShape: ['component-decimal_scaling', 'component-min_max_scaling', 'component-z_score_scaling']\n",
      "\t\tNonNullTabularDatasetShape: ['component-drop_rows_with_missing_values', 'component-mean_imputation']\n",
      "\tTotal combinations: 24\n",
      "\t\tCombination 1 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 48: workflow_48_DescriptionIntent_84e676dd_a3a9_4840_b1a1_e4005df6e900\n",
      "\t\tCombination 2 / 24: ['component-random_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 49: workflow_49_DescriptionIntent_d6304b12_298a_4ae5_b5ed_273122834f1a\n",
      "\t\tCombination 3 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 50: workflow_50_DescriptionIntent_e1756acf_8203_4f14_81d0_20b8faa537e2\n",
      "\t\tCombination 4 / 24: ['component-random_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 51: workflow_51_DescriptionIntent_b916a39a_32f5_4360_86f2_3b2deb1caa39\n",
      "\t\tCombination 5 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 52: workflow_52_DescriptionIntent_a4d04cc2_6de8_41c0_9042_df216ecd4cb8\n",
      "\t\tCombination 6 / 24: ['component-random_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 53: workflow_53_DescriptionIntent_d5f3deaa_cd71_4b34_8075_8c3da092f86b\n",
      "\t\tCombination 7 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 54: workflow_54_DescriptionIntent_297e236a_a052_4901_ba1c_6d1c06fe22b2\n",
      "\t\tCombination 8 / 24: ['component-random_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 55: workflow_55_DescriptionIntent_716b31fb_8291_42d3_a1e4_6284d0d22a3b\n",
      "\t\tCombination 9 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 56: workflow_56_DescriptionIntent_e91273bc_ae2d_482e_ad14_d316e16fb442\n",
      "\t\tCombination 10 / 24: ['component-random_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 57: workflow_57_DescriptionIntent_0433a7f8_29f0_4d23_92da_f6bf91ab258b\n",
      "\t\tCombination 11 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 58: workflow_58_DescriptionIntent_3705ffc4_fc0a_4ac6_8272_715d7c1f881b\n",
      "\t\tCombination 12 / 24: ['component-random_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 59: workflow_59_DescriptionIntent_511c354f_d47a_44d7_a9db_fc99e4feccbc\n",
      "\t\tCombination 13 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 60: workflow_60_DescriptionIntent_df4bc2a4_5180_46a6_bca8_8b4a3ba76b61\n",
      "\t\tCombination 14 / 24: ['component-top_k_absolute_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 61: workflow_61_DescriptionIntent_44183ae6_2a88_43a7_aafd_e31becaadea5\n",
      "\t\tCombination 15 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 62: workflow_62_DescriptionIntent_c1193b45_bb6b_4bbd_8b8d_0593ea9e653b\n",
      "\t\tCombination 16 / 24: ['component-top_k_absolute_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 63: workflow_63_DescriptionIntent_fd803dad_8863_470a_a948_609d719630b0\n",
      "\t\tCombination 17 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 64: workflow_64_DescriptionIntent_f23cc8ae_7a64_44ae_8faf_5bd6b7af9cd8\n",
      "\t\tCombination 18 / 24: ['component-top_k_absolute_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 65: workflow_65_DescriptionIntent_e3aef586_836d_48ac_b9ec_5843ea464104\n",
      "\t\tCombination 19 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 66: workflow_66_DescriptionIntent_54f6ba76_8a75_48f7_832c_99ab8862d8dc\n",
      "\t\tCombination 20 / 24: ['component-top_k_relative_train_test_split', 'component-decimal_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 67: workflow_67_DescriptionIntent_e11d8326_ee6b_48bb_a97d_435c8de54ec9\n",
      "\t\tCombination 21 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 68: workflow_68_DescriptionIntent_8a926362_58dc_4513_8630_1452589cd6c7\n",
      "\t\tCombination 22 / 24: ['component-top_k_relative_train_test_split', 'component-min_max_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 69: workflow_69_DescriptionIntent_c3841820_f7bc_41e9_9585_161a96a18b91\n",
      "\t\tCombination 23 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-drop_rows_with_missing_values']\n",
      "\t\tWorkflow 70: workflow_70_DescriptionIntent_b6a8398a_e5d9_44d0_b2d0_6ba61bf9bf51\n",
      "\t\tCombination 24 / 24: ['component-top_k_relative_train_test_split', 'component-z_score_scaling', 'component-mean_imputation']\n",
      "\t\tWorkflow 71: workflow_71_DescriptionIntent_061ed942_f808_4b5f_912d_cf3847692ffd\n"
     ]
    }
   ],
   "source": [
    "dataset, problem, intent_params, intent_iri = get_intent_info(intent_graph)\n",
    "folder = f'./workflows/{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "if log:\n",
    "    print(f'Dataset: {dataset.fragment}')\n",
    "    print(f'Problem: {problem.fragment}')\n",
    "    print(f'Intent params: {intent_params}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "comps = get_potential_implementations(ontology, problem, [x['param'] for x in intent_params])\n",
    "components = [\n",
    "    (c, impl, inputs)\n",
    "    for impl, inputs in comps\n",
    "    for c in get_implementation_components(ontology, impl)\n",
    "]\n",
    "if log:\n",
    "    for component, implementation, inputs in components:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "        for im_input in inputs:\n",
    "            print(f'\\tInput: {[x.fragment for x in im_input]}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "workflow_order = 0\n",
    "\n",
    "split_components = [\n",
    "    dabox.term('component-random_absolute_train_test_split'),\n",
    "    dabox.term('component-random_relative_train_test_split'),\n",
    "    dabox.term('component-top_k_absolute_train_test_split'),\n",
    "    dabox.term('component-top_k_relative_train_test_split'),\n",
    "]\n",
    "\n",
    "for component, implementation, inputs in components[1:]:\n",
    "    if log:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "    shapes_to_satisfy = identify_data_io(ontology, inputs)\n",
    "    assert shapes_to_satisfy is not None and len(shapes_to_satisfy) > 0\n",
    "    if log:\n",
    "        print(f'\\tData input: {[x.fragment for x in shapes_to_satisfy]}')\n",
    "\n",
    "    unsatisfied_shapes = [shape for shape in shapes_to_satisfy if\n",
    "                          not satisfies_shape(ontology, ontology, shape, dataset)]\n",
    "\n",
    "    available_transformations = {\n",
    "        shape: find_components_to_satisfy_shape(ontology, shape, only_learners=True)\n",
    "        for shape in unsatisfied_shapes\n",
    "    }\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tUnsatisfied shapes: ')\n",
    "        for shape, comps in available_transformations.items():\n",
    "            print(f'\\t\\t{shape.fragment}: {[x.fragment for x in comps]}')\n",
    "\n",
    "    transformation_combinations = list(itertools.product(split_components, *available_transformations.values()))\n",
    "    # TODO - check if the combination is valid and whether further transformations are needed\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tTotal combinations: {len(transformation_combinations)}')\n",
    "\n",
    "    for i, transformation_combination in enumerate(transformation_combinations):\n",
    "        if log:\n",
    "            print(\n",
    "                f'\\t\\tCombination {i + 1} / {len(transformation_combinations)}: {[x.fragment for x in transformation_combination]}')\n",
    "\n",
    "        workflow_name = f'workflow_{workflow_order}_{intent_iri.fragment}_{uuid.uuid4()}'.replace('-', '_')\n",
    "        wg, w = build_workflow_train_test(workflow_name, ontology, dataset, component, transformation_combination[0],\n",
    "                                          transformation_combination[1:])\n",
    "        if log:\n",
    "            print(f'\\t\\tWorkflow {workflow_order}: {w.fragment}')\n",
    "        wg.serialize(f'{folder}{workflow_name}.ttl', format='turtle')\n",
    "        workflow_order += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T19:41:15.427267300Z",
     "start_time": "2023-07-15T19:40:38.398994300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
