{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import uuid\n",
    "from typing import Tuple, Any, List\n",
    "\n",
    "from owlrl import DeductiveClosure, OWLRL_Semantics\n",
    "from pyshacl import validate\n",
    "\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline generation algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ontology = get_ontology_graph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Obtain Intent Information functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:50.458486Z",
     "start_time": "2023-07-15T13:00:50.270487Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_intent_iri(intent_graph):\n",
    "    intent_iri_query = f\"\"\"\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "SELECT ?iri\n",
    "WHERE {{\n",
    "    ?iri a dtbox:Intent .\n",
    "}}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(intent_iri_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['iri']\n",
    "\n",
    "\n",
    "def get_intent_dataset_problem(intent_graph, intent_iri):\n",
    "    dataset_problem_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?dataset ?problem\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:Intent .\n",
    "        {intent_iri.n3()} dtbox:overData ?dataset .\n",
    "        {intent_iri.n3()} dtbox:tackles ?problem .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(dataset_problem_query).bindings[0]\n",
    "    return result['dataset'], result['problem']\n",
    "\n",
    "\n",
    "def get_intent_params(intent_graph, intent_iri):\n",
    "    params_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?param ?value\n",
    "    WHERE {{\n",
    "        {intent_iri.n3()} a dtbox:UserIntent .\n",
    "        {intent_iri.n3()} dtbox:usingParameter ?param_value .\n",
    "        ?param_value dtbox:forParameter ?param .\n",
    "        ?param_value dtbox:has_value ?value .\n",
    "    }}\n",
    "\"\"\"\n",
    "    result = intent_graph.query(params_query).bindings\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_intent_info(intent_graph, intent_iri=None) -> Tuple[Any, Any, List[Any], Any]:\n",
    "    if not intent_iri:\n",
    "        intent_iri = get_intent_iri(intent_graph)\n",
    "\n",
    "    dataset, problem = get_intent_dataset_problem(intent_graph, intent_iri)\n",
    "    params = get_intent_params(intent_graph, intent_iri)\n",
    "\n",
    "    return dataset, problem, params, intent_iri"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Obtain Loader functions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T13:00:55.379704800Z",
     "start_time": "2023-07-15T13:00:55.359705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nee3a405cc25d4048bd7801084c90ab48b101 http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://diviloper.dev/ontology#IOSpec\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#hasTag https://diviloper.dev/ontology/shapes#LabeledTabularDatasetShape\n",
      "nee3a405cc25d4048bd7801084c90ab48b101 https://diviloper.dev/ontology#has_position 0\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Obtain Main component dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def get_implementation_input_specs(ontology, implementation):\n",
    "    input_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesInput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(input_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def get_implementation_output_specs(ontology, implementation):\n",
    "    output_spec_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?shape\n",
    "        WHERE {{\n",
    "            {implementation.n3()} dtbox:specifiesOutput ?spec .\n",
    "            ?spec a dtbox:IOSpec ;\n",
    "                dtbox:hasTag ?shape ;\n",
    "                dtbox:has_position ?position .\n",
    "            ?shape a dtbox:DataTag .\n",
    "        }}\n",
    "        ORDER BY ?position\n",
    "    \"\"\"\n",
    "    results = ontology.query(output_spec_query).bindings\n",
    "    shapes = [flatten_shape(ontology, result['shape']) for result in results]\n",
    "    return shapes\n",
    "\n",
    "\n",
    "def flatten_shape(graph, shape):\n",
    "    if (shape, SH['and'], None) in graph:\n",
    "        subshapes_query = f\"\"\"\n",
    "            PREFIX sh: <{SH}>\n",
    "            PREFIX rdf: <{RDF}>\n",
    "\n",
    "            SELECT ?subshape\n",
    "            WHERE {{\n",
    "                {shape.n3()} sh:and ?andNode .\n",
    "                ?andNode rdf:rest*/rdf:first ?subshape .\n",
    "            }}\n",
    "        \"\"\"\n",
    "        subshapes = graph.query(subshapes_query).bindings\n",
    "\n",
    "        return [x for subshape in subshapes for x in flatten_shape(graph, subshape['subshape'])]\n",
    "    else:\n",
    "        return [shape]\n",
    "\n",
    "\n",
    "def get_potential_implementations(ontology, problem_iri, intent_parameters=None) -> List[Tuple[Any, List[Any]]]:\n",
    "    if intent_parameters is None:\n",
    "        intent_parameters = []\n",
    "    intent_params_match = [f'dtbox:hasParameter {param.n3()} ;' for param in intent_parameters]\n",
    "    intent_params_separator = '            \\n'\n",
    "    main_implementation_query = f\"\"\"\n",
    "    PREFIX dtbox: <{dtbox}>\n",
    "    SELECT ?implementation\n",
    "    WHERE {{\n",
    "        ?implementation a dtbox:Implementation ;\n",
    "            {intent_params_separator.join(intent_params_match)}\n",
    "            dtbox:implements ?algorithm .\n",
    "        ?algorithm a dtbox:Algorithm ;\n",
    "            dtbox:solves ?problem .\n",
    "        ?problem dtbox:subProblemOf* {problem_iri.n3()} .\n",
    "        FILTER NOT EXISTS{{\n",
    "            ?implementation a dtbox:ApplierImplementation.\n",
    "        }}\n",
    "    }}\n",
    "\"\"\"\n",
    "    results = ontology.query(main_implementation_query).bindings\n",
    "    implementations = [result['implementation'] for result in results]\n",
    "\n",
    "    implementations_with_shapes = [\n",
    "        (implementation, get_implementation_input_specs(ontology, implementation))\n",
    "        for implementation in implementations]\n",
    "\n",
    "    return implementations_with_shapes\n",
    "\n",
    "\n",
    "def get_component_implementation(ontology, component):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:hasImplementation ?implementation .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    assert len(result) == 1\n",
    "    return result[0]['implementation']\n",
    "\n",
    "\n",
    "def get_implementation_components(ontology, implementation) -> List[Any]:\n",
    "    components_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?component\n",
    "        WHERE {{\n",
    "            ?component dtbox:hasImplementation {implementation.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(components_query).bindings\n",
    "    return [result['component'] for result in results]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T15:38:15.234633400Z",
     "start_time": "2023-07-15T15:38:15.207634300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_components_to_satisfy_shape(ontology, shape, only_appliers=False):\n",
    "    implementation_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?implementation\n",
    "        WHERE {{\n",
    "            ?implementation a dtbox:{'Applier' if only_appliers else ''}Implementation ;\n",
    "                dtbox:specifiesOutput ?spec .\n",
    "            ?spec dtbox:hasTag {shape.n3()} .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    result = ontology.query(implementation_query).bindings\n",
    "    implementations = [x['implementation'] for x in result]\n",
    "    components = [c\n",
    "                  for implementation in implementations\n",
    "                  for c in get_implementation_components(ontology, implementation)]\n",
    "    return components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def identify_data_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            if (io_shape, SH.targetClass, dmop.TabularDataset) in ontology:\n",
    "                return i if return_index else io_shapes\n",
    "\n",
    "\n",
    "def identify_model_io(ontology: Graph, ios: List[Any], return_index=False) -> Any:\n",
    "    for i, io_shapes in enumerate(ios):\n",
    "        for io_shape in io_shapes:\n",
    "            query = f'''\n",
    "    PREFIX sh: <{SH}>\n",
    "    PREFIX rdfs: <{RDFS}>\n",
    "    PREFIX ddata: <{dd}>\n",
    "\n",
    "    ASK {{\n",
    "      {io_shape.n3()} sh:targetClass ?targetClass .\n",
    "\n",
    "      ?targetClass rdfs:subClassOf* ddata:Model .\n",
    "    }}\n",
    "'''\n",
    "            if ontology.query(query).askAnswer:\n",
    "                return i if return_index else io_shapes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def satisfies_shape(data_graph, shacl_graph, shape, focus):\n",
    "    conforms, g, report = validate(data_graph, shacl_graph=shacl_graph, validate_shapes=[shape], focus=focus)\n",
    "    return conforms\n",
    "\n",
    "\n",
    "def get_shape_target_class(ontology, shape):\n",
    "    return ontology.query(f\"\"\"\n",
    "        PREFIX sh: <{SH}>\n",
    "        SELECT ?targetClass\n",
    "        WHERE {{\n",
    "            <{shape}> sh:targetClass ?targetClass .\n",
    "        }}\n",
    "    \"\"\").bindings[0]['targetClass']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def get_implementation_parameters(ontology, implementation) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value\n",
    "        WHERE {{\n",
    "            <{implementation}> dtbox:hasParameter ?parameter .\n",
    "            ?parameter dtbox:hasDefaultValue ?value ;\n",
    "                       dtbox:has_position ?order .\n",
    "        }}\n",
    "        ORDER BY ?order\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: param['value'] for param in results}\n",
    "\n",
    "\n",
    "def get_component_overriden_parameters(ontology, component) -> dict:\n",
    "    parameters_query = f\"\"\"\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?parameter ?value\n",
    "        WHERE {{\n",
    "            {component.n3()} dtbox:overridesParameter ?parameterValue .\n",
    "            ?parameterValue dtbox:forParameter ?parameter ;\n",
    "                       dtbox:withValue ?value .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    results = ontology.query(parameters_query).bindings\n",
    "    return {param['parameter']: param['value'] for param in results}\n",
    "\n",
    "\n",
    "def get_component_parameters(ontology, component) -> dict:\n",
    "    implementation = get_component_implementation(ontology, component)\n",
    "    implementation_params = get_implementation_parameters(ontology, implementation)\n",
    "    component_params = get_component_overriden_parameters(ontology, component)\n",
    "    implementation_params.update(component_params)\n",
    "    return implementation_params\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T15:39:19.867925900Z",
     "start_time": "2023-07-15T15:39:19.843924700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_step(graph, pipeline, task_name, implementation, parameters, order, previous_task=None, inputs=None,\n",
    "             outputs=None):\n",
    "    if outputs is None:\n",
    "        outputs = []\n",
    "    if inputs is None:\n",
    "        inputs = []\n",
    "    step = dw.term(task_name)\n",
    "    graph.add((pipeline, dtbox.hasStep, step))\n",
    "    graph.add((step, RDF.type, dtbox.Step))\n",
    "    graph.add((step, dtbox.runs, implementation))\n",
    "    graph.add((step, dtbox.has_position, Literal(order)))\n",
    "    for i, input in enumerate(inputs):\n",
    "        in_node = BNode()\n",
    "        graph.add((in_node, RDF.type, dtbox.IO))\n",
    "        graph.add((in_node, dtbox.hasData, input))\n",
    "        graph.add((in_node, dtbox.has_position, Literal(i)))\n",
    "        graph.add((step, dtbox.hasInput, in_node))\n",
    "    for o, output in enumerate(outputs):\n",
    "        out_node = BNode()\n",
    "        graph.add((out_node, RDF.type, dtbox.IO))\n",
    "        graph.add((out_node, dtbox.hasData, output))\n",
    "        graph.add((out_node, dtbox.has_position, Literal(o)))\n",
    "        graph.add((step, dtbox.hasOutput, out_node))\n",
    "    for param in parameters:\n",
    "        param_value = BNode()\n",
    "        graph.add((step, dtbox.hasParameterValue, param_value))\n",
    "        graph.add((param_value, dtbox.forParameter, param['parameter']))\n",
    "        if 'value' in param:\n",
    "            graph.add((param_value, dtbox.has_value, param['value']))\n",
    "        else:\n",
    "            graph.add((param_value, dtbox.has_value, Literal('REQUIRES USER INPUT')))\n",
    "    if previous_task:\n",
    "        if isinstance(previous_task, list):\n",
    "            for previous in previous_task:\n",
    "                graph.add((previous, dtbox.followedBy, step))\n",
    "        else:\n",
    "            graph.add((previous_task, dtbox.followedBy, step))\n",
    "    return step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_component_transformations(ontology, component) -> List:\n",
    "    transformation_query = f'''\n",
    "        PREFIX dtbox: <{dtbox}>\n",
    "        SELECT ?transformation\n",
    "        WHERE {{\n",
    "            <{component}> dtbox:hasTransformation ?transformation_list .\n",
    "            ?transformation_list rdf:rest*/rdf:first ?transformation .\n",
    "        }}\n",
    "    '''\n",
    "    transformations = ontology.query(transformation_query).bindings\n",
    "    return [x['transformation'] for x in transformations]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def copy_subgraph(source_graph: Graph, source_node: URIRef, destination_graph: Graph, destination_node: URIRef,\n",
    "                  replace_nodes: bool = True):\n",
    "    visited_nodes = set()\n",
    "    nodes_to_visit = [source_node]\n",
    "    mappings = {source_node: destination_node}\n",
    "\n",
    "    while nodes_to_visit:\n",
    "        current_node = nodes_to_visit.pop()\n",
    "        visited_nodes.add(current_node)\n",
    "        for predicate, object in source_graph.predicate_objects(current_node):\n",
    "            if predicate == OWL.sameAs:\n",
    "                continue\n",
    "            if replace_nodes and isinstance(object, IdentifiedNode):\n",
    "                if predicate == RDF.type or object in dmop:\n",
    "                    mappings[object] = object\n",
    "                else:\n",
    "                    if object not in visited_nodes:\n",
    "                        nodes_to_visit.append(object)\n",
    "                    if object not in mappings:\n",
    "                        mappings[object] = BNode()\n",
    "                destination_graph.add((mappings[current_node], predicate, mappings[object]))\n",
    "            else:\n",
    "                destination_graph.add((mappings[current_node], predicate, object))\n",
    "\n",
    "\n",
    "def annotate_io_with_spec(ontology: Graph, workflow_graph: Graph, io: URIRef, io_spec: List[URIRef]):\n",
    "    for spec in io_spec:\n",
    "        io_spec_class = next(ontology.objects(spec, SH.targetClass, True), None)\n",
    "        if io_spec_class is None or (io, RDF.type, io_spec_class) in workflow_graph:\n",
    "            continue\n",
    "        workflow_graph.add((io, RDF.type, io_spec_class))\n",
    "\n",
    "\n",
    "def annotate_ios_with_specs(ontology: Graph, workflow_graph: Graph, io: List[URIRef], specs: List[List[URIRef]]):\n",
    "    assert len(io) == len(specs), 'Number of IOs and specs must be the same'\n",
    "    for io, spec in zip(io, specs):\n",
    "        annotate_io_with_spec(ontology, workflow_graph, io, spec)\n",
    "\n",
    "\n",
    "def run_copy_transformation(ontology: Graph, workflow_graph: Graph, transformation, inputs, outputs):\n",
    "    input_index = next(ontology.objects(transformation, dtbox.copy_input, True)).value\n",
    "    output_index = next(ontology.objects(transformation, dtbox.copy_output, True)).value\n",
    "    input = inputs[input_index - 1]\n",
    "    output = outputs[output_index - 1]\n",
    "\n",
    "    copy_subgraph(workflow_graph, input, workflow_graph, output)\n",
    "\n",
    "\n",
    "def run_component_transformation(ontology: Graph, workflow_graph: Graph, component, inputs, outputs,\n",
    "                                 parameters):\n",
    "    transformations = get_component_transformations(ontology, component)\n",
    "    for transformation in transformations:\n",
    "        if (transformation, RDF.type, dtbox.CopyTransformation) in ontology:\n",
    "            run_copy_transformation(ontology, workflow_graph, transformation, inputs, outputs)\n",
    "        else:\n",
    "            prefixes = f'''\n",
    "PREFIX dtbox: <{dtbox}>\n",
    "PREFIX rdf: <{RDF}>\n",
    "PREFIX rdfs: <{RDFS}>\n",
    "PREFIX owl: <{OWL}>\n",
    "PREFIX xsd: <{XSD}>\n",
    "PREFIX dmop: <{dmop}>\n",
    "'''\n",
    "            query = next(ontology.objects(transformation, dtbox.transformation_query, True)).value\n",
    "            query = prefixes + query\n",
    "            for i in range(len(inputs)):\n",
    "                query = query.replace(f'$input{i + 1}', f'{inputs[i].n3()}')\n",
    "            for i in range(len(outputs)):\n",
    "                query = query.replace(f'$output{i + 1}', f'{outputs[i].n3()}')\n",
    "            for i in range(len(parameters)):\n",
    "                query = query.replace(f'$param{i + 1}', f'{parameters[i].toPython()}')\n",
    "                query = query.replace(f'$parameter{i + 1}', f'{parameters[i].toPython()}')\n",
    "            workflow_graph.update(query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def step_name(workflow_name, task_order, implementation):\n",
    "    return f'{workflow_name}-step_{task_order}_{implementation.fragment.replace(\"-\", \"_\")}'\n",
    "\n",
    "\n",
    "def build_workflow_train_test(workflow_name, ontology, dataset, main_component, split_component, transformations):\n",
    "    workflow_graph = get_graph()\n",
    "    workflow = dw.term(workflow_name)\n",
    "    workflow_graph.add((workflow, RDF.type, dtbox.Workflow))\n",
    "    task_order = 0\n",
    "\n",
    "    dataset_node = dw.term(f'{workflow_name}-original_dataset')\n",
    "\n",
    "    copy_subgraph(ontology, dataset, workflow_graph, dataset_node)\n",
    "\n",
    "    split_step_name = step_name(workflow_name, task_order, split_component)\n",
    "    split_outputs = [dw[f'{split_step_name}-output_train'], dw[f'{split_step_name}-output_test']]\n",
    "    split_parameters = get_component_parameters(ontology, split_component)\n",
    "    split_step = add_step(workflow_graph, workflow,\n",
    "                          split_step_name,\n",
    "                          split_component,\n",
    "                          split_parameters,\n",
    "                          task_order,\n",
    "                          None,\n",
    "                          [dataset_node],\n",
    "                          split_outputs)\n",
    "    run_component_transformation(ontology, workflow_graph, split_component,\n",
    "                                 [dataset_node], split_outputs,\n",
    "                                 [p['value'] for p in split_parameters])\n",
    "\n",
    "    task_order += 1\n",
    "\n",
    "    train_dataset_node = split_outputs[0]\n",
    "    test_dataset_node = split_outputs[1]\n",
    "\n",
    "    previous_train_step = split_step\n",
    "    previous_test_step = split_step\n",
    "\n",
    "    for test_implementation in [*transformations, main_component]:\n",
    "        train_counterpart = next(ontology.objects(test_implementation, dtbox.hasLearner, True), test_implementation)\n",
    "        same = train_counterpart == test_implementation\n",
    "\n",
    "        train_step_name = step_name(workflow_name, task_order, train_counterpart)\n",
    "        test_step_name = step_name(workflow_name, task_order + 1, test_implementation)\n",
    "\n",
    "        train_input_specs = get_implementation_input_specs(ontology, train_counterpart)\n",
    "        train_input_data_index = identify_data_io(ontology, train_input_specs, return_index=True)\n",
    "        train_transformation_inputs = [dw[f'{train_step_name}-input_{i}'] for i in range(len(train_input_specs))]\n",
    "        train_transformation_inputs[train_input_data_index] = train_dataset_node\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_inputs,\n",
    "                                train_input_specs)\n",
    "\n",
    "        train_output_specs = get_implementation_output_specs(ontology, train_counterpart)\n",
    "        train_output_model_index = identify_model_io(ontology, train_output_specs, return_index=True)\n",
    "        train_output_data_index = identify_data_io(ontology, train_output_specs, return_index=True)\n",
    "        train_transformation_outputs = [dw[f'{train_step_name}-output_{i}'] for i in range(len(train_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, train_transformation_outputs,\n",
    "                                train_output_specs)\n",
    "\n",
    "        train_step = add_step(workflow_graph, workflow,\n",
    "                              train_step_name,\n",
    "                              train_counterpart, [], task_order, previous_train_step, train_transformation_inputs,\n",
    "                              train_transformation_outputs)\n",
    "\n",
    "        previous_train_step = train_step\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, train_counterpart, train_transformation_inputs,\n",
    "                                     train_transformation_outputs, [])\n",
    "\n",
    "        if train_output_data_index is not None:\n",
    "            train_dataset_node = train_transformation_outputs[train_output_data_index]\n",
    "\n",
    "        task_order += 1\n",
    "\n",
    "        test_input_specs = get_implementation_input_specs(ontology, test_implementation)\n",
    "        test_input_data_index = identify_data_io(ontology, test_input_specs, return_index=True)\n",
    "        test_input_model_index = identify_model_io(ontology, test_input_specs, return_index=True)\n",
    "        test_transformation_inputs = [dw[f'{test_step_name}-input_{i}'] for i in range(len(test_input_specs))]\n",
    "        test_transformation_inputs[test_input_data_index] = test_dataset_node\n",
    "        test_transformation_inputs[test_input_model_index] = train_transformation_outputs[train_output_model_index]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_inputs,\n",
    "                                test_input_specs)\n",
    "\n",
    "        test_output_specs = get_implementation_output_specs(ontology, test_implementation)\n",
    "        test_output_data_index = identify_data_io(ontology, test_output_specs, return_index=True)\n",
    "        test_transformation_outputs = [dw[f'{test_step_name}-output_{i}'] for i in range(len(test_output_specs))]\n",
    "        annotate_ios_with_specs(ontology, workflow_graph, test_transformation_outputs,\n",
    "                                test_output_specs)\n",
    "\n",
    "        previous_test_steps = [previous_test_step, train_step] if not same else [previous_test_step]\n",
    "        test_step = add_step(workflow_graph, workflow,\n",
    "                             test_step_name,\n",
    "                             test_implementation, [], task_order, previous_test_steps, test_transformation_inputs,\n",
    "                             test_transformation_outputs)\n",
    "\n",
    "        run_component_transformation(ontology, workflow_graph, test_implementation, test_transformation_inputs,\n",
    "                                     test_transformation_outputs, [])\n",
    "\n",
    "        test_dataset_node = test_transformation_outputs[test_output_data_index]\n",
    "        previous_test_step = test_step\n",
    "        task_order += 1\n",
    "\n",
    "    return workflow_graph, workflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intent_graph = get_graph()\n",
    "ins = Namespace('https://diviloper.dev/intent#')\n",
    "intent_graph.add((ins.DescriptionIntent, RDF.type, dtbox.Intent))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.overData, dd.term('penguins.csv')))\n",
    "intent_graph.add((ins.DescriptionIntent, dtbox.tackles, dabox.Description))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: penguins.csv\n",
      "Problem: Description\n",
      "Intent params: []\n",
      "-------------------------------------------------\n",
      "Component: component-decision_tree_learner (implementation-decision_tree_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape']\n",
      "Component: component-hypertangent_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape']\n",
      "Component: component-polynomial_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape']\n",
      "Component: component-rbf_svm_learner (implementation-svm_learner)\n",
      "\tInput: ['LabeledTabularDatasetShape', 'NormalizedTabularDatasetShape']\n",
      "-------------------------------------------------\n",
      "Component: component-decision_tree_learner (implementation-decision_tree_learner)\n",
      "\tData input: ['LabeledTabularDatasetShape']\n",
      "\tUnsatisfied shapes: \n",
      "\tTransformation combinations: \n",
      "\t\t['component-random_absolute_train_test_split']\n",
      "\t\t['component-random_relative_train_test_split']\n",
      "\t\t['component-top_k_absolute_train_test_split']\n",
      "\t\t['component-top_k_relative_train_test_split']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 62\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transformation_combination \u001B[38;5;129;01min\u001B[39;00m transformation_combinations:\n\u001B[0;32m     61\u001B[0m     workflow_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkflow_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworkflow_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mintent_iri\u001B[38;5;241m.\u001B[39mfragment\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muuid\u001B[38;5;241m.\u001B[39muuid4()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 62\u001B[0m     wg, w \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_workflow_train_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworkflow_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43montology\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomponent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformation_combination\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformation_combinations\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m log:\n\u001B[0;32m     64\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mWorkflow \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworkflow_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mw\u001B[38;5;241m.\u001B[39mfragment\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[25], line 26\u001B[0m, in \u001B[0;36mbuild_workflow_train_test\u001B[1;34m(workflow_name, ontology, dataset, main_component, split_component, transformations)\u001B[0m\n\u001B[0;32m     17\u001B[0m split_parameters \u001B[38;5;241m=\u001B[39m get_implementation_parameters(ontology, split_component)\n\u001B[0;32m     18\u001B[0m split_step \u001B[38;5;241m=\u001B[39m add_step(workflow_graph, workflow,\n\u001B[0;32m     19\u001B[0m                       split_step_name,\n\u001B[0;32m     20\u001B[0m                       split_component,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m                       [dataset_node],\n\u001B[0;32m     25\u001B[0m                       split_outputs)\n\u001B[1;32m---> 26\u001B[0m \u001B[43mrun_component_transformation\u001B[49m\u001B[43m(\u001B[49m\u001B[43montology\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworkflow_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_component\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m                             \u001B[49m\u001B[43m[\u001B[49m\u001B[43mdataset_node\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m                             \u001B[49m\u001B[43m[\u001B[49m\u001B[43mp\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalue\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msplit_parameters\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m task_order \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     32\u001B[0m train_dataset_node \u001B[38;5;241m=\u001B[39m split_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[1;32mIn[28], line 73\u001B[0m, in \u001B[0;36mrun_component_transformation\u001B[1;34m(ontology, workflow_graph, component, inputs, outputs, parameters)\u001B[0m\n\u001B[0;32m     71\u001B[0m     query \u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$param\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparameters[i]\u001B[38;5;241m.\u001B[39mtoPython()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     72\u001B[0m     query \u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$parameter\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparameters[i]\u001B[38;5;241m.\u001B[39mtoPython()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 73\u001B[0m \u001B[43mworkflow_graph\u001B[49m\u001B[38;5;241m.\u001B[39mupdate(query)\n",
      "Cell \u001B[1;32mIn[28], line 73\u001B[0m, in \u001B[0;36mrun_component_transformation\u001B[1;34m(ontology, workflow_graph, component, inputs, outputs, parameters)\u001B[0m\n\u001B[0;32m     71\u001B[0m     query \u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$param\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparameters[i]\u001B[38;5;241m.\u001B[39mtoPython()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     72\u001B[0m     query \u001B[38;5;241m=\u001B[39m query\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$parameter\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparameters[i]\u001B[38;5;241m.\u001B[39mtoPython()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 73\u001B[0m \u001B[43mworkflow_graph\u001B[49m\u001B[38;5;241m.\u001B[39mupdate(query)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\231.9225.15\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\231.9225.15\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dataset, problem, intent_params, intent_iri = get_intent_info(intent_graph)\n",
    "\n",
    "if log:\n",
    "    print(f'Dataset: {dataset.fragment}')\n",
    "    print(f'Problem: {problem.fragment}')\n",
    "    print(f'Intent params: {intent_params}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "comps = get_potential_implementations(ontology, problem, [x['param'] for x in intent_params])\n",
    "components = [\n",
    "    (c, impl, inputs)\n",
    "    for impl, inputs in comps\n",
    "    for c in get_implementation_components(ontology, impl)\n",
    "]\n",
    "if log:\n",
    "    for component, implementation, inputs in components:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "        for im_input in inputs:\n",
    "            print(f'\\tInput: {[x.fragment for x in im_input]}')\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "workflow_order = 0\n",
    "\n",
    "split_components = [\n",
    "    dabox.term('component-random_absolute_train_test_split'),\n",
    "    dabox.term('component-random_relative_train_test_split'),\n",
    "    dabox.term('component-top_k_absolute_train_test_split'),\n",
    "    dabox.term('component-top_k_relative_train_test_split'),\n",
    "]\n",
    "\n",
    "for component, implementation, inputs in components:\n",
    "    if log:\n",
    "        print(f'Component: {component.fragment} ({implementation.fragment})')\n",
    "    shapes_to_satisfy = identify_data_io(ontology, inputs)\n",
    "    assert shapes_to_satisfy is not None and len(shapes_to_satisfy) > 0\n",
    "    if log:\n",
    "        print(f'\\tData input: {[x.fragment for x in shapes_to_satisfy]}')\n",
    "\n",
    "    unsatisfied_shapes = [shape for shape in shapes_to_satisfy if\n",
    "                          not satisfies_shape(ontology, ontology, shape, dataset)]\n",
    "\n",
    "    available_transformations = {\n",
    "        shape: find_components_to_satisfy_shape(ontology, shape, only_appliers=True)\n",
    "        for shape in unsatisfied_shapes\n",
    "    }\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tUnsatisfied shapes: ')\n",
    "        for shape, comps in available_transformations.items():\n",
    "            print(f'\\t\\t{shape.fragment}: {[x.fragment for x in comps]}')\n",
    "\n",
    "    transformation_combinations = list(itertools.product(split_components, *available_transformations.values()))\n",
    "    # TODO - check if the combination is valid and whether further transformations are needed\n",
    "\n",
    "    if log:\n",
    "        print(f'\\tTransformation combinations: ')\n",
    "        for combination in transformation_combinations:\n",
    "            print(f'\\t\\t{[x.fragment for x in combination]}')\n",
    "\n",
    "    for transformation_combination in transformation_combinations:\n",
    "        workflow_name = f'workflow_{workflow_order}_{intent_iri.fragment}_{uuid.uuid4()}'.replace('-', '_')\n",
    "        wg, w = build_workflow_train_test(workflow_name, ontology, dataset, component, transformation_combination[0],\n",
    "                                          transformation_combinations[1:])\n",
    "        if log:\n",
    "            print(f'\\t\\tWorkflow {workflow_order}: {w.fragment}')\n",
    "        wg.serialize(f'{workflow_name}.ttl', format='turtle')\n",
    "        workflow_order += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(ontology.triples((None, dtbox.hasImplementation, None)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
